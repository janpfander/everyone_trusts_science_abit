---
title             : "Quasi-universal acceptance of basic science in the US"
shorttitle        : "Does anyone mistrust basic science?"

header-includes:  | # to prevent floats from moving past certain points (for the appendix)
  \usepackage{placeins} 
  
author: 
  - name          : ""
    affiliation   : ""

affiliation:
  - id            : ""
    institution   : ""
    
abstract: |
  XX
  
keywords          : 



floatsintext      : yes
linenumbers       : no 
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "doc" # "doc" for nice look, "man" for manuscripty
output            : papaja::apa6_doc # "_doc" for word; however, note that some of the features of kableExtra are not available for word and will yield errors. For example to knit to word, you'll have to comment out all "add_header_above()" functions for model output tables, or e.g. set always allow html as true in the yaml heading

always_allow_html: true

appendix:
  - "appendix_exp1.Rmd"
  - "appendix_exp2.Rmd"
  - "appendix_exp3.Rmd"
  - "appendix_exp4.Rmd"

bibliography: references.bib
---

```{r setup, include=FALSE}
# Figure out output format
is_docx <- knitr::pandoc_to("docx") | knitr::pandoc_to("odt")
is_latex <- knitr::pandoc_to("latex")
is_html <- knitr::pandoc_to("html")

# Word-specific things
table_format <- ifelse(is_docx, "huxtable", "kableExtra")  # Huxtable tables
conditional_dpi <- ifelse(is_docx, 300, 300)  # Higher DPI
conditional_align <- ifelse(is_docx, "default", "center")  # Word doesn't support align

# Knitr options
knitr::opts_chunk$set(
  echo = FALSE, warning = FALSE, message = FALSE,
  # tidy.opts = list(width.cutoff = 120),  # Code width
  # fig.retina = 3, dpi = conditional_dpi,
  # fig.width = 7, fig.asp = 0.618,
  # fig.align = conditional_align, out.width = "100%",
  fig.path = "output/figures/",
  cache.path = "output/_cache/",
  fig.process = function(x) {  # Remove "-1" from figure names
    x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
    if (file.rename(x, x2)) x2 else x
  },
  options(scipen = 99999999)  # Prevent scientific notation
)

# R options
options(
  width = 90,  # Output width
  dplyr.summarise.inform = FALSE,  # Turn off dplyr's summarize() auto messages
  knitr.kable.NA = "",  # Make NAs blank in kables
  kableExtra.latex.load_packages = FALSE,  # Don't add LaTeX preamble stuff
  modelsummary_factory_default = table_format,  # Set modelsummary backend
  modelsummary_format_numeric_latex = "plain"  # Don't use siunitx
)
```

```{r packages, include=FALSE}
# load required packages
library("papaja")      # For APA style manuscript   
library("lme4")        # model specification / estimation
library("lmerTest")    # provides p-values in the output
library("tidyverse")   # data wrangling and visualisation
library("afex")        # anova and deriving p-values from lmer
library("broom")       # extracting data from model fits 
library("broom.mixed") # extracting data from mixed models
library("metafor")     # doing mata analysis
library("patchwork")   # put several plots together
library("ggridges")    # for plots
library("gghalves")    # for plots
library("ggbeeswarm")  # Special distribution-shaped point jittering
library("knitr")       # for tables
library("kableExtra")  # also for tables
library("ggpubr")      # for combining plots with ggarrange() 
library("grid")        # for image plots   
library("gridExtra")   # for image plots
library("png")         # for image plots
library("modelsummary") # for regression tables
library("ggrepel") # for better random patterns in plots
```

```{r functions}
# load plot theme
source("functions/plot_theme.R") 

# load other functions
source("functions/own_functions.R")
```

# Introduction

Trust in science is related to many desirable outcomes, from acceptance of anthropogenic climate change (ref) or vaccination (ref) to following recommendations during COVID [@alganTrustScientistsTimes2021], which suggests that trust in science was the most important predictor of these behaviors).

Unfortunately, people who report a high degree of trust in science are a minority in most countries, and they are outnumbered by people who have low trust in science in many areas, e.g., most of Africa and significant parts of Asia [@wellcomeglobalmonitorWellcomeGlobalMonitor2018; @wellcomeglobalmonitorWellcomeGlobalMonitor2020]. Moreover, trust in science has recently been declining in some countries [@alganTrustScientistsTimes2021; although see @wellcomeglobalmonitorPublicTrustScientists2021], including the US [@brianAmericansTrustScientists2023], where it is also increasingly polarizing [@gauchatPoliticizationSciencePublic2012; @krauseTrendsAmericansTrust2019a; @liPolarizationPublicTrust2022]. Rejection of science can take the more extreme form of conspiracy theories, many of which question the scientific consensus on vaccination, climate change, and even the shape of the Earth.

What does this apparent rejection of science actually entail? Do people who say they do not trust science, or who believe in conspiracy theories at odds with well-established science, reject most of science? Or, on the contrary, do they object to a few specific facets of science, while still accepting the overwhelming majority of basic science? Answering this question has theoretical implications: if some people reject science wholesale, then it’s possible that their distrust of science causes their rejection of specific scientific theories, or facilitates their acceptance of conspiracy theories; by contrast, if everyone trusts most of science, then the mistrust of specific facets of science is more likely to be post-hoc, a rationalization of other beliefs or behaviors. The present question also has practical implications: many communication attempts leverage the scientific consensus [e.g. on vaccination, climate change, etc., for review, see @vanstekelenburgScientificConsensusCommunicationContested2022; see also @veckalov27countryTestCommunicating2024]. These attempts are more likely to be successful if everyone trusts basic science than if some people reject science wholesale.

Studies of science knowledge provide relevant evidence, as people who answer science knowledge questions correctly presumably trust scientists in this respect. The average level of science knowledge is quite low, even in countries with extensive science education. For example, only 55% in the US (in 2014) and only 46% in the EU (in 2005) knew that antibiotics kill only bacteria and not viruses [@committeeonscienceliteracyandpublicperceptionofscienceScienceLiteracyConcepts2016]. However, these results only provide a lower bound on how much people trust basic science, as participants who do not provide the correct answer might accept it when it is pointed out to them.

This relative lack of trust in science could mean different things. It could mean that some people reject science wholesale, or at least to a significant extent. Or it could mean that they maintain a high degree of trust in most of science, and only question specific results that happen to contradict specific other beliefs they have (e.g. creationism), or beliefs that are associated with disliked behaviors (e.g. vaccination, acting for climate change).

We answer this question by looking at how much Americans accept basic scientific knowledge. We pay special attention to two groups of participants: people who say they don’t trust science (much, or at all), and people who endorse science-related conspiracy theories (CTs), or have a CT mindset, since this has been linked with a low degree of trust in science, and that many CTs imply a mistrust of at least some science (vaccination, hydroxychloroquine, etc.).

We briefly review work on trust in science, and science knowledge, before turning to work on CT and trust in science.

## Trust in science

Across the globe, most people say they trust science at least to some extent. In 2018, the Wellcome Global Monitor (WGM) surveyed of over 140'000 people in over 140 countries on trust in science [@wellcomeglobalmonitorWellcomeGlobalMonitor2018]. In 2020, during the first year of the Covid pandemic, a follow up survey was made in 113 countries, involving 119'000 participants [@wellcomeglobalmonitorWellcomeGlobalMonitor2020]. Across all countries, in 2018, 32% of participants said they trust science "a lot" (41% in 2020), 45% trust "some" science (39% in 2020), and only 13% percent trust science "not much/ not at all" (13% in 2020), while 10% indicated "don't know" (7% in 2020)[^1].

[^1]: <https://wellcome.org/reports/wellcome-global-monitor-covid-19/2020#gid=6acd&pid=0>\>

Trust in science also appears to be relatively stable in the US [@funkPublicConfidenceScientists2020]. However, there are some potentially worrying trends: Recently, trust in US has dropped [@brianAmericansTrustScientists2023], and some evidence suggests increasing polarization of trust in science along political lines [@gauchatPoliticizationSciencePublic2012]. For example, the partisan divide on the question of whether climate change is a major threat to society has been growing in the last decade[^2] [@kennedyWhatDataSays2023]. A recent study suggests that by some measure, only about half of the US population believes in anthropogenic climate change, and that individuals do mostly not seem to change their mind on on the issue [@mottaChangingMindsChanging2021].

[^2]: <https://www.pewresearch.org/short-reads/2023/08/09/what-the-data-says-about-americans-views-of-climate-change/>

Trust in science is typically measured in three ways: either asking explicitly about general trust in science or scientists, by asking about science attitudes (e.g. support for public funding of science), or asking questions about specific, typically contentious, science topics (e.g. vaccines, climate change, GMOs). It is unclear to which extent these measures capture trust in basic science. To assess this, we can look at the literature on science knowledge.

[introduce SK as: that gives us a lower bound of how much people trust basic science]

## Science knowledge

Much of the research on public understanding of science measures to which extent people have basic science knowledge (e.g. "Does the Earth go around the Sun, or does the Sun go around the Earth?"). For us, these surveys provide a lower bound to how much people trust basic science: people who give the right answer (and not by chance) supposedly trust science on that question. Unfortunately, that lower bound has often been indeed pretty low: researchers have pointed out the deficit in basic science knowledge [@millerPublicUnderstandingAttitudes2004; @millerMeasurementCivicScientific1998a]. For example, only 55% in the US (in 2014) and only 46% in the EU (in 2005) knew that antibiotics kill only bacteria and not viruses [@committeeonscienceliteracyandpublicperceptionofscienceScienceLiteracyConcepts2016]. A question that remains is whether people do not provide the right answers because they are not familiar with the scientific consensus, or because they explicitly reject that consensus.

[To do: Make a descriptive overview of science knowledge across time/countries.]

## CT and TiS

Recently, the relationship of one group to trust in science (TiS) has been particularly scrutinized: conspiracy theorists (CTists), either defined as people who endorse specific conspiracy theories, or as people who have a conspiracy mindset [@rutjensConspiracyBeliefsScience2022; @vranicDidMyOwn2022]. CTists reject specific science knowledge (related to their CT), and they tend to trust science less on the whole. What remains unclear is to which extent CTists (dis)trust basic science.

## The present studies

In a series of four pre-registered online studies (total n = 782), we asked US participants questions about well-established scientific facts. For each question, we asked participants what they thought the correct answer was (testing their knowledge of science), we informed them of the scientifically accepted correct answer, and asked them whether they accepted it (measuring their trust in basic science). We also measured participants’ trust in science using standard measures, as well as their beliefs in various conspiracy theories and their tendency to engage in conspiratorial thinking. The four studies, including materials, hypotheses, and analyses, were pre-registered and all materials and data are accessible via the Open Science Framework (OSF)). The differences between the four studies are summarized presently, and the methods are detailed below.

### Materials

In Study 1, we used questions drawn from questionnaires of scientific knowledge (e.g. "Are electrons smaller, larger, or the same size as atoms? [Smaller; Same size; Larger]"), supplemented by a ‘trick’ question ("Where do trees mainly draw the materials with which they create their mass? [Earth; Water; Air]"; correct answer: Air). In Studies 2 and 3, this last question was removed. The scientific facts used in Studies 1 to 3 represent long-established and basic knowledge. In Study 4 we used more recent much less basic scientific discoveries (e.g. "How many more glial cells are there in the brain in comparison with neurons, as established in 2016? [The same amount; Twice as many; Tenth as many]"; correct answer: The same amount).

### Presentation of the scientific consensus

In Study 1, we simply told participants that they would be provided with the scientifically consensual answer. However, for participants to accept this answer, they must not only trust science, but also trust us (that we are presenting them with the actual scientifically consensual answer). To remove this issue, in Studies 2 to 3, we presented participants with a short explanation of the correct answer, as well as links to three sources per answer (e.g. Wikipedia, National Geographic or NASA). In Study 4, as the topics were more complex, we did not provide an explanation, but we provided two sources per answer.

### Measure of acceptance of the scientific consensus

In Study 1, we simply look at whether participants say that they accept the scientifically consensual answer. In the subsequent studies, we asked participants to explain why they disagreed with the scientific consensus. This revealed that a number of participants had made a mistake (misunderstanding, selecting the wrong answer). As a result, in Studies 3 and 4, participants who have indicated that they rejected the scientifically consensual answer were offered the option to revise their answer, or to keep rejecting it.

### Additional questions

In Studies 3 and 4, we attempted to understand how some people who say they do not trust science still accept scientifically consensual answers, by asking them whether they accepted the answers on the basis of trust in science or because they had independently verified them.

### Samples

Studies 1 and 2 were conducted on the standard sample of US participants recruited on the platform Prolific Academic. In order to increase the share of participants with low trust in science, and who endorse conspiracy theories, Studies 3 and 4 used the same platform, but only recruited participants who had declared previously being skeptical of vaccination.

### Hypotheses

The main goal of the present studies is descriptive: to find out whether participants who report not trusting science, or who believe in conspiracy theories, still accept most well-established scientific facts. However, we also tested two directional hypotheses (pre-registered as research questions in the Study 1):

**H1: Higher trust in science is associated with more science knowledge and more acceptance of the scientific consensus**

**H2: Higher conspiracy thinking/belief is associated with less science knowledge and less acceptance of the scientific consensus**

# Methods

```{r exp1}
# Analyze data of experiments and store results

# Experiment 1

# read the cleaned long version data set
exp1_long <- read_csv("exp_1/data/cleaned_long.csv")

# read wide version data set
exp1_wide <- read_csv("exp_1/data/cleaned_wide.csv") 

# Research questions

# RQ3
cor_trust_knowledge <- cor.test(exp1_wide$wgm_sciencegeneral, exp1_wide$avg_knowledge) %>% text_ready()

cor_trust_acceptance <- cor.test(exp1_wide$wgm_sciencegeneral, exp1_wide$avg_acceptance) %>% text_ready()

# RQ4
cor_conspiracy_knowledge <- cor.test(exp1_wide$BCTI_avg, exp1_wide$avg_knowledge) %>% text_ready()

cor_conspiracy_acceptance <- cor.test(exp1_wide$BCTI_avg, exp1_wide$avg_acceptance)%>% text_ready()

# Exploratory: Acceptance and trust in science/conspiracy thinking conditional on false answers only
exp1_false_knowledge <- exp1_long %>% 
  # make numeric versions
  mutate(acceptance_num = ifelse(acceptance == "Yes", 1, 0)
  ) %>% 
  group_by(id, knowledge) %>% 
  # calculate by-participant averages
  summarize(
    n = n(),
    n_accepted = sum(acceptance_num),
    avg_acceptance  = sum(acceptance_num)/n(), 
    wgm_sciencegeneral = mean(wgm_sciencegeneral), 
    BCTI_avg = mean(BCTI_avg), 
    # add additonal measures for robustness checks
    CMQ_avg = mean(CMQ_avg), 
    SICBS = mean(SICBS),
    wgm_scientists = mean(wgm_scientists), 
    pew = mean(pew)
  ) %>% 
  # filter to only false responses
  filter(knowledge == FALSE) %>% 
  ungroup()

false_answers_cor_trust_acceptance <- cor.test(exp1_false_knowledge$wgm_sciencegeneral, exp1_false_knowledge$avg_acceptance) %>% text_ready()

false_answers_cor_conspiracy_acceptance <- cor.test(exp1_false_knowledge$BCTI_avg, exp1_false_knowledge$avg_acceptance) %>% text_ready()


# extract descriptives for inline reporting
exp1_descriptives <- list(
  # Demographics
  n_subj = n_distinct(exp1_wide$id),
  gender = exp1_wide %>% group_by(gender) %>% summarize(n = n_distinct(id)) %>% split(.$gender),
  age = exp1_wide %>% summarize(across(age,list(mean = mean, median = median, sd = sd), 
                                       .names = "{.fn}")) %>% rounded_numbers(),
  # RQ1 & RQ2
  means = exp1_wide %>% 
    summarize(across(c(avg_knowledge, avg_acceptance), 
                     list(mean = mean, sd = sd), 
                     .names = "{.col}_{.fn}")) %>%
    rename_with(~sub("^avg_", "", .), everything()) %>% 
    mutate_if(is.numeric, round, digits = 2) %>% 
    mutate(across(ends_with("_mean"), ~paste0(.*100, " %"))),
  means_numeric = exp1_wide %>% 
    summarize(across(c(avg_knowledge, avg_acceptance,), 
                     list(mean = mean, sd = sd), 
                     .names = "{.col}_{.fn}")) %>%
    rename_with(~sub("^avg_", "", .), everything()),
  # RQ3
  cor_trust_knowledge = cor_trust_knowledge,
  cor_trust_acceptance = cor_trust_acceptance,
  # RQ4
  cor_conspiracy_knowledge = cor_conspiracy_knowledge,
  cor_conspiracy_acceptance = cor_conspiracy_acceptance, 
  # Exploratory
  # acceptance and knowledge
  conditional_acceptance = exp1_long %>% 
    group_by(knowledge, acceptance) %>% 
    count() %>% 
    group_by(knowledge) %>% 
    mutate (share = n/sum(n), 
            # rename knowledge values
            knowledge = ifelse(knowledge == FALSE, "false", "true")) %>% 
    mutate_if(is.numeric, round, digits = 3) %>% 
    mutate (share = paste0(share*100, " %")) %>% 
    super_split(knowledge, acceptance),
  # acceptance and trust in science/conspiracy thinking
  false_answers_cor_trust_acceptance = false_answers_cor_trust_acceptance,
  false_answers_cor_conspiracy_acceptance = false_answers_cor_conspiracy_acceptance
)
```

```{r exp2}
# Analyze data of experiments and store results

# Experiment 2

# read the cleaned long version data set
exp2_long <- read_csv("exp_2/data/cleaned_long.csv")

# read wide version data set
exp2_wide <- read_csv("exp_2/data/cleaned_wide.csv") 

# read coded justifications
exp2_justifications <- read_csv("exp_2/data/justifications_clean.csv")

# Research questions

# H1a
cor_trust_knowledge <- cor.test(exp2_wide$wgm_sciencegeneral, exp2_wide$avg_knowledge) %>% text_ready()

# H2a
cor_conspiracy_knowledge <- cor.test(exp2_wide$BCTI_avg, exp2_wide$avg_knowledge) %>% text_ready()

# Exploratory: Acceptance and trust in science/conspiracy thinking conditional on false answers only
exp2_false_knowledge <- exp2_long %>% 
  # make numeric versions
  mutate(acceptance_num = ifelse(acceptance == "Yes", 1, 0)
  ) %>% 
  group_by(id, knowledge) %>% 
  # calculate by-participant averages
  summarize(
    n = n(),
    n_accepted = sum(acceptance_num),
    avg_acceptance  = sum(acceptance_num)/n(), 
    wgm_sciencegeneral = mean(wgm_sciencegeneral), 
    BCTI_avg = mean(BCTI_avg), 
    # add additonal measures for robustness checks
    CMQ_avg = mean(CMQ_avg), 
    SICBS = mean(SICBS),
    wgm_scientists = mean(wgm_scientists), 
    pew = mean(pew)
  ) %>% 
  # filter to only false responses
  filter(knowledge == FALSE) %>% 
  ungroup()

# H1b
# conditional on false answers
false_answers_cor_trust_acceptance <- cor.test(exp2_false_knowledge$wgm_sciencegeneral, exp2_false_knowledge$avg_acceptance) %>% text_ready()
# pooled across all cases (in line with all other studies' analysis)
cor_trust_acceptance <- cor.test(exp2_wide$wgm_sciencegeneral, exp2_wide$avg_acceptance) %>% text_ready()

# H2b
# conditional on false answers
false_answers_cor_conspiracy_acceptance <- cor.test(exp2_false_knowledge$BCTI_avg, exp2_false_knowledge$avg_acceptance) %>% text_ready()
# pooled across all cases (in line with all other studies' analysis)
cor_conspiracy_acceptance <- cor.test(exp2_wide$BCTI_avg, exp2_wide$avg_acceptance) %>% text_ready()


# extract descriptives for inline reporting
exp2_descriptives <- list(
  # Demographics
  n_subj = n_distinct(exp2_wide$id),
  gender = exp2_wide %>% group_by(gender) %>% summarize(n = n_distinct(id)) %>% split(.$gender),
  age = exp2_wide %>% summarize(across(age,list(mean = mean, median = median, sd = sd), 
                                       .names = "{.fn}")) %>% rounded_numbers(),
  # RQ1 & RQ2
  means = exp2_wide %>% 
    summarize(across(c(avg_knowledge, avg_acceptance), 
                     list(mean = mean, sd = sd), 
                     .names = "{.col}_{.fn}")) %>%
    rename_with(~sub("^avg_", "", .), everything()) %>% 
    mutate_if(is.numeric, round, digits = 2) %>% 
    mutate(across(ends_with("_mean"), ~paste0(.*100, " %"))) ,
  means_numeric = exp2_wide %>% 
    summarize(across(c(avg_knowledge, avg_acceptance,), 
                     list(mean = mean, sd = sd), 
                     .names = "{.col}_{.fn}")) %>%
    rename_with(~sub("^avg_", "", .), everything()),
  # H1
  # a
  cor_trust_knowledge = cor_trust_knowledge,
  # b (conditional on false responses)
  false_answers_cor_trust_acceptance = false_answers_cor_trust_acceptance,
  # b (all cases)
  cor_trust_acceptance = cor_trust_acceptance,
  # H2
  # a
  cor_conspiracy_knowledge = cor_conspiracy_knowledge,
  # b (conditional on false responses)
  false_answers_cor_conspiracy_acceptance = false_answers_cor_conspiracy_acceptance,
  # b(all cases)
  cor_conspiracy_acceptance = cor_conspiracy_acceptance, 
  # Exploratory
  # acceptance and knowledge
  conditional_acceptance = exp2_long %>% 
    group_by(knowledge, acceptance) %>% 
    count() %>% 
    group_by(knowledge) %>% 
    mutate (share = n/sum(n), 
            # rename knowledge values
            knowledge = ifelse(knowledge == FALSE, "false", "true")) %>% 
    mutate_if(is.numeric, round, digits = 3) %>% 
    mutate (share = paste0(share*100, " %")) %>% 
    super_split(knowledge, acceptance),
  # Justifications
  justifications_n = nrow(exp2_justifications), 
  justifications_n_participants = n_distinct(exp2_justifications$id),
  justification_by_category = exp2_justifications %>%
    group_by(category)%>%
    summarize(n = n(), 
              n_subjects = n_distinct(id)) %>%
    mutate(share = n/sum(n)) %>%
    mutate_if(is.numeric, round, digits=3) %>% 
    mutate(share = paste0(share*100, "%")) %>% 
    split(.$category)
)
```

```{r exp3}
# Analyze data of experiments and store results

# Experiment 3

# read the cleaned long version data set
exp3_long <- read_csv("exp_3/data/cleaned_long.csv")

# read wide version data set
exp3_wide <- read_csv("exp_3/data/cleaned_wide.csv") 

# read coded justifications
exp3_justifications <- read_csv("exp_3/data/justifications_clean.csv")

# Research questions

# H1
cor_trust_knowledge <- cor.test(exp3_wide$wgm_sciencegeneral, exp3_wide$avg_knowledge,
  use = "complete.obs") %>% text_ready()
cor_trust_acceptance <- cor.test(exp3_wide$wgm_sciencegeneral, exp3_wide$avg_acceptance,
  use = "complete.obs") %>% text_ready()

# H2
cor_conspiracy_knowledge <- cor.test(exp3_wide$BCTI_avg, exp3_wide$avg_knowledge,
  use = "complete.obs") %>% text_ready()
cor_conspiracy_acceptance <- cor.test(exp3_wide$BCTI_avg, exp3_wide$avg_acceptance,
  use = "complete.obs") %>% text_ready()

# Exploratory: Acceptance and trust in science/conspiracy thinking conditional on false answers only
exp3_false_knowledge <- exp3_long %>% 
  # make numeric versions
  mutate(acceptance_num = ifelse(acceptance == "Yes", 1, 0)
  ) %>% 
  group_by(id, knowledge) %>% 
  # calculate by-participant averages
  summarize(
    n = n(),
    n_accepted = sum(acceptance_num),
    avg_acceptance  = sum(acceptance_num)/n(), 
    wgm_sciencegeneral = mean(wgm_sciencegeneral), 
    BCTI_avg = mean(BCTI_avg), 
    # add additonal measures for robustness checks
    CMQ_avg = mean(CMQ_avg), 
    SICBS = mean(SICBS),
    wgm_scientists = mean(wgm_scientists), 
    pew = mean(pew)
  ) %>% 
  # filter to only false responses
  filter(knowledge == FALSE) %>% 
  ungroup()

# H1b conditional on false answers
false_answers_cor_trust_acceptance <- cor.test(exp3_false_knowledge$wgm_sciencegeneral, exp3_false_knowledge$avg_acceptance,
  use = "complete.obs") %>% 
  text_ready()

# H2b conditional on false answers
false_answers_cor_conspiracy_acceptance <- cor.test(exp3_false_knowledge$BCTI_avg, exp3_false_knowledge$avg_acceptance,
  use = "complete.obs") %>% 
  text_ready()

# Exploratory: Differences by reason for trust
trust_by_reason_model <- run_regression(exp3_wide, dependent_var = "wgm_sciencegeneral", independent_var = "reason_agreement")

acceptance_by_reason_model <-run_regression(exp3_wide, dependent_var = "avg_acceptance", independent_var = "reason_agreement")

# extract descriptives for inline reporting
exp3_descriptives <- list(
  # Demographics
  n_subj = n_distinct(exp3_wide$id),
  n_subject_by_outcome = exp3_wide %>% 
    summarize(across(c("wgm_sciencegeneral", "reason_agreement", "BCTI_avg"), 
                     ~sum(!is.na(.x)
                     )
    )
    ),
  gender = exp3_wide %>% group_by(gender) %>% summarize(n = n_distinct(id)) %>% split(.$gender),
  age = exp3_wide %>% summarize(across(age,list(mean = mean, median = median, sd = sd), 
                                       .names = "{.fn}")) %>% rounded_numbers(),
  # RQ1 & RQ2
  means = exp3_wide %>% 
    summarize(across(c(avg_knowledge, avg_acceptance, avg_acceptance_initial), 
                     list(mean = mean, sd = sd), 
                     .names = "{.col}_{.fn}")) %>%
    rename_with(~sub("^avg_", "", .), everything()) %>% 
    mutate_if(is.numeric, round, digits = 2) %>% 
  mutate(across(ends_with("_mean"), ~paste0(.*100, " %"))) ,
  means_numeric = exp3_wide %>% 
    summarize(across(c(avg_knowledge, avg_acceptance, avg_acceptance_initial), 
                     list(mean = mean, sd = sd), 
                     .names = "{.col}_{.fn}")) %>%
    rename_with(~sub("^avg_", "", .), everything()),
  # H1
  # a
  cor_trust_knowledge = cor_trust_knowledge,
  # b (conditional on false responses)
  false_answers_cor_trust_acceptance = false_answers_cor_trust_acceptance,
  # b (all cases)
  cor_trust_acceptance = cor_trust_acceptance,
  # H2
  # a
  cor_conspiracy_knowledge = cor_conspiracy_knowledge,
  # b (conditional on false responses)
  false_answers_cor_conspiracy_acceptance = false_answers_cor_conspiracy_acceptance,
  # b(all cases)
  cor_conspiracy_acceptance = cor_conspiracy_acceptance, 
  # Exploratory
  # acceptance and knowledge
  conditional_acceptance = exp3_long %>% 
    pivot_longer(c(acceptance, acceptance_initial), 
                 names_to = "measure", 
                 values_to = "acceptance") %>% 
    group_by(knowledge, measure, acceptance) %>% 
    count() %>% 
    group_by(knowledge, measure) %>% 
    mutate (share = n/sum(n), 
            # rename knowledge values
            knowledge = ifelse(knowledge == FALSE, "false", "true")) %>% 
    mutate_if(is.numeric, round, digits = 3) %>% 
    mutate (share = paste0(share*100, " %")) %>% 
    super_split(knowledge,  measure, acceptance),
  # Justifications
  justifications_n = nrow(exp3_justifications), 
  justifications_n_participants = n_distinct(exp3_justifications$id),
  justification_by_category = exp3_justifications %>%
    group_by(category)%>%
    summarize(n = n(), 
              n_subjects = n_distinct(id)) %>%
    mutate(share = n/sum(n)) %>%
    mutate_if(is.numeric, round, digits=3) %>% 
    mutate(share = paste0(share*100, "%")) %>% 
    split(.$category),
  # Consensus acceptance
  acceptance_n = exp3_wide %>% 
    filter(!is.na(reason_agreement)) %>% 
    nrow(.),
  acceptance_by_reason = exp3_wide %>% 
    group_by(reason_agreement) %>% 
    summarize(n = n()) %>%
    drop_na(reason_agreement) %>% 
    mutate(share = n/sum(n)) %>%
    mutate_if(is.numeric, round, digits=3) %>% 
    mutate(share = paste0(share*100, "%")) %>% 
    split(.$reason_agreement),
  reason_followup_n = exp3_wide %>% 
    filter(!is.na(reason_followup)) %>% 
    nrow(.),
  # Exploratory: Differences by reason for trust
  reason_means = exp3_wide %>% 
    group_by(reason_agreement) %>% 
    summarize(across(c(wgm_sciencegeneral, avg_acceptance), 
                     list(mean = mean, sd = sd), 
                     .names = "{.col}_{.fn}")) %>%
    rename_with(~sub("^avg_", "", .), everything()) %>% 
    mutate_if(is.numeric, round, digits = 2) %>% 
    mutate(across(acceptance_mean, ~paste0(.*100, " %"))) %>% 
    split(.$reason_agreement), 
  reason_trust_model = trust_by_reason_model %>% split(.$term), 
  reason_acceptance_model = acceptance_by_reason_model %>% split(.$term)
)
```

```{r exp4}
# Analyze data of experiments and store results

# Experiment 4

# read the cleaned long version data set
exp4_long <- read_csv("exp_4/data/cleaned_long.csv")

# read wide version data set
exp4_wide <- read_csv("exp_4/data/cleaned_wide.csv") 

# read coded justifications
exp4_justifications <- read_csv("exp_4/data/justifications_clean.csv")

# Research questions

# H1
cor_trust_knowledge <- cor.test(exp4_wide$wgm_sciencegeneral, exp4_wide$avg_knowledge,
  use = "complete.obs") %>% text_ready()
cor_trust_acceptance <- cor.test(exp4_wide$wgm_sciencegeneral, exp4_wide$avg_acceptance,
  use = "complete.obs") %>% text_ready()

# H2
cor_conspiracy_knowledge <- cor.test(exp4_wide$BCTI_avg, exp4_wide$avg_knowledge,
  use = "complete.obs") %>% text_ready()
cor_conspiracy_acceptance <- cor.test(exp4_wide$BCTI_avg, exp4_wide$avg_acceptance,
  use = "complete.obs") %>% text_ready()

# Exploratory: Acceptance and trust in science/conspiracy thinking conditional on false answers only
exp4_false_knowledge <- exp4_long %>% 
  # make numeric versions
  mutate(acceptance_num = ifelse(acceptance == "Yes", 1, 0)
  ) %>% 
  group_by(id, knowledge) %>% 
  # calculate by-participant averages
  summarize(
    n = n(),
    n_accepted = sum(acceptance_num),
    avg_acceptance  = sum(acceptance_num)/n(), 
    wgm_sciencegeneral = mean(wgm_sciencegeneral), 
    BCTI_avg = mean(BCTI_avg), 
    # add additonal measures for robustness checks
    CMQ_avg = mean(CMQ_avg), 
    SICBS = mean(SICBS),
    wgm_scientists = mean(wgm_scientists), 
    pew = mean(pew)
  ) %>% 
  # filter to only false responses
  filter(knowledge == FALSE) %>% 
  ungroup()

# H1b conditional on false answers
false_answers_cor_trust_acceptance <- cor.test(exp4_false_knowledge$wgm_sciencegeneral, exp4_false_knowledge$avg_acceptance,
  use = "complete.obs") %>% 
  text_ready()

# H2b conditional on false answers
false_answers_cor_conspiracy_acceptance <- cor.test(exp4_false_knowledge$BCTI_avg, exp4_false_knowledge$avg_acceptance,
  use = "complete.obs") %>% 
  text_ready()

# Exploratory: Differences by reason for trust
trust_by_reason_model <- run_regression(exp4_wide, dependent_var = "wgm_sciencegeneral", independent_var = "reason_agreement")

acceptance_by_reason_model <- run_regression(exp4_wide, dependent_var = "avg_acceptance", independent_var = "reason_agreement")

clicks_by_reason_model <- run_regression(exp4_wide, dependent_var = "sum_link_clicks", independent_var = "reason_agreement")

# extract descriptives for inline reporting
exp4_descriptives <- list(
  # Demographics
  n_subj = n_distinct(exp4_wide$id),
  n_subject_by_outcome = exp4_wide %>% 
    summarize(across(c("wgm_sciencegeneral", "reason_agreement", "BCTI_avg"), 
                     ~sum(!is.na(.x)
                     )
    )
    ),
  gender = exp4_wide %>% group_by(gender) %>% summarize(n = n_distinct(id)) %>% split(.$gender),
  age = exp4_wide %>% summarize(across(age,list(mean = mean, median = median, sd = sd), 
                                       .names = "{.fn}")) %>% rounded_numbers(),
  # RQ1 & RQ2
  means = exp4_wide %>% 
    summarize(across(c(avg_knowledge, avg_acceptance, avg_acceptance_initial), 
                     list(mean = mean, sd = sd), 
                     .names = "{.col}_{.fn}")) %>%
    rename_with(~sub("^avg_", "", .), everything()) %>% 
    mutate_if(is.numeric, round, digits = 2) %>% 
    mutate(across(ends_with("_mean"), ~paste0(.*100, " %"))) ,
  means_numeric = exp4_wide %>% 
    summarize(across(c(avg_knowledge, avg_acceptance, avg_acceptance_initial), 
                     list(mean = mean, sd = sd), 
                     .names = "{.col}_{.fn}")) %>%
    rename_with(~sub("^avg_", "", .), everything()),
  # H1
  # a
  cor_trust_knowledge = cor_trust_knowledge,
  # b (conditional on false responses)
  false_answers_cor_trust_acceptance = false_answers_cor_trust_acceptance,
  # b (all cases)
  cor_trust_acceptance = cor_trust_acceptance,
  # H2
  # a
  cor_conspiracy_knowledge = cor_conspiracy_knowledge,
  # b (conditional on false responses)
  false_answers_cor_conspiracy_acceptance = false_answers_cor_conspiracy_acceptance,
  # b(all cases)
  cor_conspiracy_acceptance = cor_conspiracy_acceptance, 
  # Exploratory
  # acceptance and knowledge
  conditional_acceptance = exp4_long %>% 
    pivot_longer(c(acceptance, acceptance_initial), 
                 names_to = "measure", 
                 values_to = "acceptance") %>% 
    group_by(knowledge, measure, acceptance) %>% 
    count() %>% 
    group_by(knowledge, measure) %>% 
    mutate (share = n/sum(n), 
            # rename knowledge values
            knowledge = ifelse(knowledge == FALSE, "false", "true")) %>% 
    mutate_if(is.numeric, round, digits = 3) %>% 
    mutate (share = paste0(share*100, " %")) %>% 
    super_split(knowledge,  measure, acceptance),
  # Justifications
  justifications_n = nrow(exp4_justifications), 
  justifications_n_participants = n_distinct(exp4_justifications$id),
  justification_by_category = exp4_justifications %>%
    group_by(category)%>%
    summarize(n = n(), 
              n_subjects = n_distinct(id)) %>%
    mutate(share = n/sum(n)) %>%
    mutate_if(is.numeric, round, digits=3) %>% 
    mutate(share = paste0(share*100, "%")) %>% 
    split(.$category),
  # Consensus acceptance
  acceptance_n = exp4_wide %>% 
    filter(!is.na(reason_agreement)) %>% 
    nrow(.),
  acceptance_by_reason = exp4_wide %>% 
    group_by(reason_agreement) %>% 
    summarize(n = n()) %>%
    drop_na(reason_agreement) %>% 
    mutate(share = n/sum(n)) %>%
    mutate_if(is.numeric, round, digits=3) %>% 
    mutate(share = paste0(share*100, "%")) %>% 
    split(.$reason_agreement),
  reason_followup_n = exp4_wide %>% 
    filter(!is.na(reason_followup)) %>% 
    nrow(.),
  # Exploratory: Differences by reason for trust
  reason_means = exp4_wide %>% 
    group_by(reason_agreement) %>% 
    summarize(across(c(wgm_sciencegeneral, avg_acceptance, sum_link_clicks), 
                     list(mean = mean, sd = sd), 
                     .names = "{.col}_{.fn}")) %>%
    rename_with(~sub("^avg_", "", .), everything()) %>% 
    mutate_if(is.numeric, round, digits = 2) %>% 
    mutate(across(acceptance_mean, ~paste0(.*100, " %"))) %>% 
    split(.$reason_agreement), 
  reason_trust_model = trust_by_reason_model %>% split(.$term), 
  reason_acceptance_model = acceptance_by_reason_model %>% split(.$term), 
  reason_clicks_model = clicks_by_reason_model %>% split(.$term)
)
```

```{r combined-data}
# combine data of all three studies
combined_data <- bind_rows(exp1_long %>% 
                             mutate(study = "1"), 
                           exp2_long %>% 
                             mutate(study = "2"), 
                           exp3_long %>% 
                             mutate(study = "3"),
                           exp4_long %>% 
                             mutate(study = "4")
                           )

# add a binned version for Conspiracy Thinking

# Define breaks 
breaks <- seq(0, 100, by = 20)
# Define labels for each bin in the desired format
labels <- paste0(c(0, 20, 40, 60, 80), "-", c(20, 40, 60, 80, 100))

combined_data <- combined_data %>% 
    # Create the binned version of CMQ_avg
  mutate(CMQ_avg_binned = cut(CMQ_avg, breaks = breaks, labels = labels, 
                              include.lowest = TRUE, right = FALSE)) 


# extract descriptives for inline reporting
combined_descriptives <- list(
  # acceptance
  mean_acceptance = combined_data %>% 
  group_by(acceptance) %>% 
  count() %>% 
    ungroup() %>% 
    mutate(share = n/sum(n)) %>% 
    mutate_if(is.numeric, round, digits = 3) %>% 
    mutate(share = paste0(share*100, " %")) %>% 
    split(.$acceptance), 
  # acceptance by trust
  acceptance_by_trust = combined_data %>% 
    group_by(acceptance, wgm_sciencegeneral) %>% 
    count() %>% 
    group_by(wgm_sciencegeneral) %>% 
    mutate(share = n/sum(n)) %>% 
    mutate_if(is.numeric, round, digits = 3) %>% 
    mutate(share = paste0(share*100, " %")) %>% 
    super_split(wgm_sciencegeneral, acceptance), 
  participants_by_trust = combined_data %>% 
  group_by(wgm_sciencegeneral) %>% 
  summarize(n_participants = n_distinct(id)) %>% 
    mutate(share = n_participants/sum(n_participants)) %>% 
    mutate_if(is.numeric, round, digits = 3) %>% 
    mutate(share = paste0(share*100, " %")) %>% 
    split(.$wgm_sciencegeneral),
  # acceptance by conspiracy thinking
  acceptance_by_conspiracy_thinking = combined_data %>% 
    group_by(acceptance, CMQ_avg_binned) %>% 
    count() %>% 
    group_by(CMQ_avg_binned) %>% 
    mutate(share = n/sum(n)) %>% 
    mutate_if(is.numeric, round, digits = 3) %>% 
    mutate(share = paste0(share*100, " %")) %>% 
    super_split(CMQ_avg_binned, acceptance),
  participants_by_conspiracy_thinking  = combined_data %>% 
    group_by(CMQ_avg_binned) %>% 
    summarize(n_participants = n_distinct(id)) %>% 
    mutate(share = n_participants/sum(n_participants)) %>% 
    mutate_if(is.numeric, round, digits = 3) %>% 
    mutate(share = paste0(share*100, " %")) %>% 
    split(.$CMQ_avg_binned),
  # acceptance by conspiracy belief
  acceptance_by_conspiracy_belief = combined_data %>% 
    # bring to long format
    pivot_longer(cols = BCTI_apollo:BCTI_evolution, 
                 names_to = "BCTI_item", values_to = "score") %>% 
    mutate(BCTI_item = sub("^BCTI_", "", BCTI_item)) %>% 
    group_by(acceptance, BCTI_item, score) %>% 
    count() %>% 
    group_by(BCTI_item, score) %>% 
    mutate(share = n/sum(n)) %>% 
    mutate_if(is.numeric, round, digits = 3) %>% 
    mutate(share = paste0(share*100, " %")) %>% 
    super_split(acceptance, BCTI_item, score),
  participants_by_conspiracy_belief  = combined_data %>% 
    # bring to long format
    pivot_longer(cols = BCTI_apollo:BCTI_evolution, 
                 names_to = "BCTI_item", values_to = "score") %>% 
    mutate(BCTI_item = sub("^BCTI_", "", BCTI_item)) %>% 
    group_by(BCTI_item, score) %>% 
    summarize(n_participants = n_distinct(id)) %>% 
    mutate(share = n_participants/sum(n_participants)) %>% 
    mutate_if(is.numeric, round, digits = 3) %>% 
    mutate(share = paste0(share*100, " %")) %>% 
    super_split(BCTI_item, score),
  # knowledge
  mean_knowledge = combined_data %>% 
    group_by(knowledge) %>% 
    count() %>% 
    ungroup() %>% 
    mutate (share = n/sum(n), 
            # rename knowledge values
            knowledge = ifelse(knowledge == FALSE, "false", "true")) %>% 
    mutate_if(is.numeric, round, digits = 3) %>% 
    mutate(share = paste0(share*100, " %")) %>% 
    split(.$knowledge)
)
```

## Deviations from preregistration

For study 2, we restricted our main hypotheses about acceptance to cases in which participants initially provided a wrong answer. The rationale was to see if trust in science or conspiracy belief were associated with a change of mind towards the consensus. We provide results on these conditional correlations--for Study 2 and for all other studies--in the ESM[^3]. However, for the analysis presented here, we proceeded as preregistered for all other studies, by reporting simple, unconditional correlations between acceptance and trust in science, or, respectively, conspiracy belief.

[^3]: Only in study 4 do we find evidence that changing one's mind towards the scientific consensus is associated with (more) trust in science (study 1: r = `r exp1_descriptives$false_answers_cor_trust_acceptance$estimate`, p `r exp1_descriptives$false_answers_cor_trust_acceptance$p.value`; study 2: r = `r exp2_descriptives$false_answers_cor_trust_acceptance$estimate`, p `r exp2_descriptives$false_answers_cor_trust_acceptance$p.value`; study 3: r = `r exp3_descriptives$false_answers_cor_trust_acceptance$estimate`, p `r exp3_descriptives$false_answers_cor_trust_acceptance$p.value`; study 4: r = `r exp4_descriptives$false_answers_cor_trust_acceptance$estimate`, p `r exp4_descriptives$false_answers_cor_trust_acceptance$p.value`) and only in study 2 evidence that it is associated with (less) conspiracy beliefs and (less) conspiracy thinking (study 1: r = `r exp1_descriptives$false_answers_cor_conspiracy_acceptance$estimate`, p `r exp1_descriptives$false_answers_cor_conspiracy_acceptance$p.value`; study 2: r = `r exp2_descriptives$false_answers_cor_conspiracy_acceptance$estimate`, p `r exp2_descriptives$false_answers_cor_conspiracy_acceptance$p.value`; study 3: r = `r exp3_descriptives$false_answers_cor_conspiracy_acceptance$estimate`, p `r exp3_descriptives$false_answers_cor_conspiracy_acceptance$p.value`; study 4: r = `r exp4_descriptives$false_answers_cor_conspiracy_acceptance$estimate`, p `r exp4_descriptives$false_answers_cor_conspiracy_acceptance$p.value`).

## Procedure

After providing their consent to participate in the study, participants were given an attention check "While watching the television, have you ever had a fatal heart attack?" [1-6; 1 = Never, 6 = Often]. All participants who did not answer "1 = Never" were excluded. Participants then read the following instructions:"We will ask you 10 questions about science. After each question, we will provide you with the scientifically consensual answer and ask whether you accept it." Next, participants answered a set of 10 basic science questions (in Study 1 they were randomly selected from a pool of 11 questions, as one ‘trick’ question had been added) in random order. After each question, participants were presented with an answer reflecting the scientific consensus, and asked whether they accepted it. In Studies 2 and 3, participants additionally saw a short explanation, partly based on explanations generated by ChatGPT, and three links to authoritative sources supporting the answer. In Study 4, we provided only two links and no explanation. Participants then answered questions on conspiracy thinking, conspiracy beliefs, and trust in science.

In Studies 2, 3, and 4, we presented participants with open-ended questions so they could explain their rejection of the scientific consensus. In Studies 3 and 4, we additionally gave participants the option to change their answer and accept the scientific consensus. Finally, at the end of Studies 3 and 4, we asked participants: "For the questions in which you agreed with the scientific consensus, would you say that...?" The answer options were: (i) "You mostly agree with the consensus because, on that question, you trust scientists", (ii) "You mostly agree with the consensus because you have been able to independently verify it", and (iii) "Other", with a text box for participants to explain. Participants who selected “You mostly agree with the consensus because you have been able to independently verify it”, were asked the open-ended follow-up question: "Could you please tell us how you independently verified the information?".

## Participants

After removing failed attention checks, we were left with a total sample size of `r exp1_descriptives$n_subj + exp2_descriptives$n_subj + exp3_descriptives$n_subj + exp4_descriptives$n_subj` (`r exp1_descriptives$n_subj` in Study 1, 6 failed attention checks; `r exp2_descriptives$n_subj` in Study 2, 11 failed attention checks; `r exp3_descriptives$n_subj` in Study 3, no failed attention checks; `r exp4_descriptives$n_subj` in Study 4, 2 failed attention checks) participants in the US via prolific. Details and demographics can be found in the online supplemental material. While samples for Studies 1 and 2 were convenience samples, Studies 3 and 4 were conducted on a sample holding vaccine-skeptic beliefs. Prolific Academic allows to select participants based on their answers to a range of questions. We picked three of these questions and only recruited participants who met our criteria for each of them:

1.  "Please describe your attitudes towards the COVID-19 (Coronavirus) vaccines: [For (I feel positively about the vaccines); Against (I feel negatively about the vaccines); Neutral (I don't have strong opinions either way); Prefer not to say"]. We selected particpants who answered "Against".
2.  "Have you received a coronavirus (COVID-19) vaccination? [Yes (at least one dose); No; Prefer not to answer]". We select only people who answered "No".
3.  "On a scale from 1-7, please rate to what extent you agree with the following statement: I believe that scheduled immunizations are safe for children. [1 (totally disagree); 2 (disagree); 3 (somewhat disagree); 4 (neither agree nor disagree); 5 (somewhat agree); 6 (agree); 7 (totally agree); rather not say]". We select only people who answered '1', '2', or '3'.

## Materials

### Scientific facts

Studies 1 to 3 used 10 facts drawn from widely used questionnaires about science knowledge [@allumScienceKnowledgeAttitudes2008; @durantPublicUnderstandingScience1989; @millerMeasurementCivicScientific1998a] sometimes referred to as the "Oxford scale" [@gauchatCulturalAuthorityScience2011]. A ‘trick’ question was added in Study 1 and removed as its wording proved unclear. Study 4 used 10 more recent scientific discoveries. Table \@ref(tab:knowledge) shows all questions and their answer options used in the four studies.

```{r knowledge}
# Function to replace special characters in all cells of a data frame
replace_special_characters <- function(df) {
  df %>%
    mutate(across(everything(), ~ gsub("\u2013|\u2014|\u2212", "-", .))) %>% # Replace en-dash, em-dash, and Unicode minus with ASCII hyphen-minus
    mutate(across(everything(), ~ iconv(., from = "UTF-8", to = "ASCII//TRANSLIT"))) # Ensure all text is ASCII
}

# Read the CSV file
items <- read_csv("exp_4/materials/overview_questions.csv")

# Replace special characters
items_cleaned <- replace_special_characters(items) %>% 
  mutate(id = 1:nrow(.)) %>%
  select(id, everything())  # Ensure 'id' is the first column

# add footnote for the tree items
items_cleaned[11, 2] <- paste0("*", 
                               items_cleaned[11, 2])
  

# Output the table
knitr::kable(items_cleaned, booktabs = T, longtable = TRUE,
    caption = "Science knowledge items", 
    full_width = T, 
    col.names = c("", "Study 1-3", "Study 4")) %>%
  kable_styling(font_size = 8) %>%
  column_spec(1, width = "2em") %>%
  column_spec(2, width = "22em")%>%
  column_spec(3, width = "22em") %>% 
  footnote(symbol = "Only used in Study 1")
```

### Conspiracy beliefs

We selected 10 science/health related conspiracy theories from the Belief in Conspiracy Theory Inventory (BCTI) [@pennycookOverconfidentlyConspiratorialConspiracy2022] (Table \@ref(tab:conspiracy)). Participants were asked: "Below is a list of events for which the official version has been disputed. For each event, we would like you to indicate to what extent you believe the cover-up version of events is true or false. [1-9; labels: 1 - completly false, 5 - unsure, 9 - completely true]".

### Conspiracy thinking

We used the four-item conspiracy mentality questionnaire (CMQ) [@bruderMeasuringIndividualDifferences2013] and the single item conspiracy beliefs scale (SICBS) [@lantianMeasuringBeliefConspiracy2016]. Details and comparisons between the scales can be found in the OSM.

```{r conspiracy, echo=FALSE}
# Create the data frame
items <- c(
  "The Apollo moon landings never happened and were staged in a Hollywood film studio.",
  "A cure for cancer was discovered years ago, but this has been suppressed by the pharmaceutical industry and the U.S. Food and Drug Administration (FDA).",
  "The spread of certain viruses and/or diseases is the result of the deliberate, concealed efforts of vested interests.",
  "The claim that the climate is changing due to emissions from fossil fuels is a hoax perpetrated by corrupt scientists who want to spend more taxpayer money on climate research.",
  "The Earth is flat (not spherical) and this fact has been covered up by scientists and vested interests.",
  "There is a causal link between vaccination and autism that has been covered up by the pharmaceutical industry.",
  "In the 1950s and 1960s more than 100 million Americans received a polio vaccine contaminated with a potentially cancer-causing virus.",
  "Proof of alien contact is being concealed from the public.",
  "Hydroxychloroquine has been demonstrated to be a safe and effective treatment of COVID and this information is being suppressed.",
  "Dinosaurs never existed, evolution is not real, and scientists have been faking the fossil record.")

conspiracy_items <- data.frame(id = 1:length(items), items = items)

# Output the table
kbl(conspiracy_items, booktabs = T, longtable = T, col.names = NULL,
    caption = "Conspiracy items", full_width = T) %>%
  kable_styling(font_size = 8) %>% 
  column_spec(1, width = "3em") %>%
  column_spec(2, width = "40em") %>% 
  footnote(general = "Participants were asked to rate their belief in the conspiracy on a scale from 1 to 9, with the labels: `1 - completely false, 5 - unsure, 9 - completely true`.",
           escape = FALSE,
           footnote_as_chunk = TRUE,
           threeparttable = TRUE)
```

### Trust in science

In all analyses reported in the main paper, we measure trust in science via a question selected from the Wellcome Global Monitor surveys [@wellcomeglobalmonitorWellcomeGlobalMonitor2018; @wellcomeglobalmonitorWellcomeGlobalMonitor2020]: "In general, would you say that you trust science a lot, some, not much, or not at all? [1 = Not at all, 2 = Not much, 3 = Some, 4 = A lot]". We chose this question as it seemed to be the most general one. In the appendix, we additionally report results for two alternative measures of trust included in our studies: Another from the WGM surveys ("How much do you trust scientists in this country? Do you trust them a lot, some, not much, or not at all? [1 = Not at all, 2 = Not much, 3 = Some, 4 = A lot]"), and one from the Pew Research Center ("How much confidence do you have in scientists to act in the best interests of the public? [1-5; 1 = No confidence at all, 5 = A great deal of confidence]"; see e.g. @funkKeyFindingsPublic2019 and used by e.g. @colognaTrustScientistsTheir2024). We selected these items so that we could compare the answers in our sample to global survey results. We find that all three items are generally highly correlated throughout all studies, and that our results reported here generally replicate when using either of the alternatives measures (with two exceptions: In Study 3, we find no correlation between acceptance and the Pew question; In Study 4, we find a correlation between knowledge and both alternative trust measures, but not with our main measure; see ESM).

# Results

(ref:summary-plot) Points represent the average share of acceptance and numbers the absolute count of participants as a function of: **A** the level of trust in science ("In general, would you say that you trust science a lot, some, not much, or not at all? [1 = Not at all, 2 = Not much, 3 = Some, 4 = A lot]"); **B** the average conspiracy thinking (CMQ, five items on a scale from 0 to 100); **C** the belief in specific conspiracy theories (i.e. participants who answered 9, "completely true", for a given theory, see Table \@ref(tab:conspiracy) for the list of the theories).

```{r summary-plot, fig.cap="(ref:summary-plot)", fig.height= 10, fig.width=10}
# make plot data for trust
plot_trust <- combined_data %>% 
  drop_na(wgm_sciencegeneral, acceptance) %>% 
  # make a labels version of trust in science
  mutate(wgm_sciencegeneral = case_when(
    wgm_sciencegeneral == 1 ~ "1 (Not at all)",
    wgm_sciencegeneral == 2 ~ "2 (Not much)",
    wgm_sciencegeneral == 3 ~ "3 (Some)",
    wgm_sciencegeneral == 4 ~ "4 (A lot)",
    TRUE ~ as.character(wgm_sciencegeneral)  # Handle any other cases (optional)
  )) %>% 
  group_by(study, wgm_sciencegeneral, acceptance) %>% 
  summarize(n = n(), 
            n_participants = n_distinct(id)) %>% 
  group_by(study, wgm_sciencegeneral) %>% 
  mutate (share = n/sum(n), 
          n_participants = max(n_participants)) %>% 
  mutate_if(is.numeric, round, digits = 3)

# make trust plot
trust <- ggplot(plot_trust %>% 
                  filter(acceptance == "Yes"), 
                aes(x = wgm_sciencegeneral, y = share, color = study, shape = study)) +
  geom_pointrange(aes(ymin = 0, ymax = share), 
                  position = position_dodge(width = 0.8), 
                  alpha = 0.5, 
                  show.legend = FALSE) +
  geom_text(aes(label = n_participants, y = share + 0.01),  # Align text labels with dodged points
            position = position_dodge(width = 0.8), 
            vjust = -0.5, 
            size = 3, 
            show.legend = FALSE) +
  #geom_point(position = "dodge", alpha = 0.5) +
  # geom_text_repel(aes(label = paste0(n_participants)),
  #                 vjust = -0.5, size = 3,
  #                 show.legend = FALSE) +
  scale_y_continuous(labels = scales::percent_format(), breaks = seq(0, 1, 0.1), limits = c(0, 1)) +
  scale_color_viridis_d(option = "plasma", end = 0.8) +
  labs(x = "Trust in science", y = "Acceptance rate of consensus", color = "Study", shape = "Study") +
  plot_theme + 
  theme(legend.position = "top")

# make plot data for conspiracy belief
plot_conspiracy <- combined_data %>% 
  # bring to long format
  pivot_longer(cols = BCTI_apollo:BCTI_evolution, 
               names_to = "BCTI_item", values_to = "score") %>% 
  # make a belief variable if participant indicates a numeric answer above the
  # a belief threshold 
  mutate(belief = ifelse(score == 9, TRUE, 
                         ifelse(is.na(score), NA, FALSE)
                         ),
         BCTI_item = sub("^BCTI_", "", BCTI_item),
         # rename BCTI items
         BCTI_item = case_when(
           BCTI_item == "apollo" ~ "Moon landing", 
           BCTI_item == "cancer" ~ "Cancer cure", 
           BCTI_item == "viruses" ~ "Viruses",
           BCTI_item == "climatechange" ~ "Climate change",
           BCTI_item == "flatearth" ~ "Flat earth",
           BCTI_item == "autism" ~ "Vaccine autism",
           BCTI_item == "polio" ~ "Vaccine cancer",
           BCTI_item == "alien" ~ "Aliens",
           BCTI_item == "hydorxychlor" ~ "Covid hydroxychloroquine",
           BCTI_item == "evolution" ~ "Evolution"
         )
         ) %>% 
  group_by(study, BCTI_item, belief, acceptance) %>% 
  summarize(n = n(),
            n_participants = n_distinct(id)) %>% 
  group_by(study, BCTI_item, belief) %>% 
  mutate (share = n/sum(n), 
          n_participants = max(n_participants)) %>% 
  mutate_if(is.numeric, round, digits = 3) %>% 
  ungroup()

# make conspiracy belief plot
conspiracy <- ggplot(plot_conspiracy %>% 
         filter(acceptance == "Yes" & belief == TRUE), 
         aes(x = BCTI_item, y = share, color = study, shape = study)) +
  geom_pointrange(aes(ymin = 0, ymax = share), 
                  position = position_dodge(width = 0.8), 
                  alpha = 0.5, 
                  show.legend = FALSE) +
  geom_text(aes(label = n_participants, y = share + 0.01),  # Align text labels with dodged points
            position = position_dodge(width = 0.8), 
            vjust = -0.5, 
            size = 3, 
            show.legend = FALSE) +
  # geom_point(position = "dodge", alpha = 0.5) +
  # geom_text_repel(aes(label = paste0(n_participants)),
  #                 vjust = -0.5, size = 3,
  #                 show.legend = FALSE) +
  scale_y_continuous(labels = scales::percent_format(), breaks = seq(0, 1, 0.1), limits = c(0, 1)) +
  scale_color_viridis_d(option = "plasma", end = 0.8) +
  labs(x = "Conspiracy theory belief", y = "Acceptance rate of consensus", color = "Study", shape = "Study") +
  plot_theme +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) 

# Conspiracy Thinking

# Define breaks for each integer from 1 to 9
breaks <- seq(0, 100, by = 20)

# Define labels for each bin in the desired format
labels <- paste0(c(0, 20, 40, 60, 80), "-", c(20, 40, 60, 80, 100))

plot_data <- combined_data %>% 
  drop_na(CMQ_avg) %>% 
  # Create the binned version of CMQ_avg
  mutate(CMQ_avg_binned = cut(CMQ_avg, breaks = breaks, labels = labels, include.lowest = TRUE, right = FALSE)) %>% 
  group_by(study, CMQ_avg_binned, acceptance) %>% 
  summarize(n = n()) %>% 
  group_by(study, CMQ_avg_binned) %>% 
  mutate (share = n/sum(n)) %>% 
  mutate_if(is.numeric, round, digits = 3)

n_participants <- combined_data %>% 
  drop_na(CMQ_avg) %>% 
  # Create the binned version of CMQ_avg
  mutate(CMQ_avg_binned = cut(CMQ_avg, breaks = breaks, labels = labels, include.lowest = TRUE, right = FALSE)) %>% 
  group_by(study, CMQ_avg_binned) %>% 
  summarize(n_participants = n_distinct(id))

plot_conspiracy_thinking <- left_join(plot_data, n_participants)

# make plot
conspiracy_thinking  <- ggplot(plot_conspiracy_thinking  %>% 
                                 filter(acceptance == "Yes"), 
                               aes(x = CMQ_avg_binned, y = share, color = study, shape = study)) +
  geom_pointrange(aes(ymin = 0, ymax = share), 
                  position = position_dodge(width = 0.8), 
                  alpha = 0.5) +
  geom_text(aes(label = n_participants, y = share + 0.01),  # Align text labels with dodged points
            position = position_dodge(width = 0.8), 
            vjust = -0.5, 
            size = 3, 
            show.legend = FALSE) +
  # geom_point(position = "dodge", alpha = 0.5) +
  # geom_text_repel(aes(label = paste0(n_participants)),
  #                 vjust = -0.5, size = 3,
  #                 show.legend = FALSE) +
  scale_color_viridis_d(option = "plasma", end = 0.8) +
  scale_y_continuous(labels = scales::percent_format(), breaks = seq(0, 1, 0.1), limits = c(0, 1.01)) +
  labs(x = "Conspiracy Thinking", y = "Acceptance rate of consensus", color = "Study", shape = "Study") +
  plot_theme + 
  theme(legend.position = "top")

# remove elements of plots for better overall plot
conspiracy_thinking <- conspiracy_thinking + 
  #rremove("ylab") +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) 
  #guides(color = "none", shape = "none") 

conspiracy <- conspiracy +
  rremove("ylab") +
  guides(color = "none", shape = "none")

trust <- trust +
  rremove("ylab") 

# common plot
patchwork <- (trust + conspiracy_thinking)/conspiracy + 
  plot_layout(guides = 'collect', widths = c(1, 1)) +
  plot_annotation(tag_levels = 'A')  & theme(legend.position = "top")

wrap_elements(patchwork) +
  labs(tag = "Rate of acceptance of consensual scientific facts") +
    theme(
    plot.tag = element_text(angle = 90, , face = "bold", size = 12),
    plot.tag.position = "left"
  )
```

The main outcome of interest is acceptance of the scientifically consensual facts presented. Overall, acceptance was very high (aggregating across all studies: `r combined_descriptives$mean_acceptance$Yes$share`; Study 1: `r exp1_descriptives$means$acceptance_mean`; Study 2: `r exp2_descriptives$means$acceptance_mean`; Study 3: `r exp3_descriptives$means$acceptance_mean`; Study 4: `r exp4_descriptives$means$acceptance_mean`). Note that this includes both participants who had previously correctly answered the knowledge question, and participants who changed their mind when presented with the scientific consensus. In Studies 3 & 4, we gave participants a second chance in case they had initially rejected the consensus, which slightly increased acceptance rates in those studies (initial acceptance in Study 3: `r exp3_descriptives$means$acceptance_initial_mean`; in Study 4: `r exp4_descriptives$means$acceptance_initial_mean`).

As shown in Figure \@ref(fig:summary-plot), these very high rates of acceptance hold for: participants who they do not trust science at all (`r combined_descriptives$participants_by_trust$'1'$share` of participants, acceptance rate of `r combined_descriptives$acceptance_by_trust$'1'$Yes$share`), participants who rank in the top two deciles of the conspiracy thinking scale (`r combined_descriptives$participants_by_conspiracy_thinking$'80-100'$share` of participants, average acceptance rate of `r combined_descriptives$acceptance_by_conspiracy_thinking$'80-100'$Yes$share`), participants who consider as "completely true" (the maximum of the 9-point scale) conspiracy theories stating that the earth is flat (`r combined_descriptives$participants_by_conspiracy_belief$flatearth$'9'$share` of participants, acceptance rate of `r combined_descriptives$acceptance_by_conspiracy_belief$Yes$flatearth$'9'$share`), or that climate change due to fossil emissions is a hoax (`r combined_descriptives$participants_by_conspiracy_belief$climatechange$'9'$share` of participants, acceptance rate of `r combined_descriptives$acceptance_by_conspiracy_belief$Yes$climatechange$'9'$share`). This doesn't merely reflect science knowledge: aggregated across all studies--participants only correctly answered `r combined_descriptives$mean_knowledge$true$share` of the questions (Study 1: `r exp1_descriptives$means$knowledge_mean`; Study 2: `r exp2_descriptives$means$knowledge_mean`; Study 3: `r exp3_descriptives$means$knowledge_mean`; Study 4: `r exp4_descriptives$means$knowledge_mean`) before they were provided with the scientifically consensual answer. Did participants who had initially gotten it wrong change their minds towards the scientific consensus? Yes. In most cases (study 1: `r exp1_descriptives$conditional_acceptance$false$Yes$share`; study 2: `r exp2_descriptives$conditional_acceptance$false$Yes$share`; study 3: `r exp3_descriptives$conditional_acceptance$false$acceptance$Yes$share`; study 4: `r exp4_descriptives$conditional_acceptance$false$acceptance$Yes$share`), participants readily accepted the scientific consensus after having initially given the wrong answer to a question. In very few cases, participants changed their mind away from the scientific consensus. These participants initially gave the correct response, but rejected the scientific consensus right after, thereby contradicting their own initial response (study 1: `r exp1_descriptives$conditional_acceptance$true$No$share`; study 2: `r exp2_descriptives$conditional_acceptance$true$No$share`; study 3: `r exp3_descriptives$conditional_acceptance$true$acceptance$No$share`; study 4: `r exp4_descriptives$conditional_acceptance$true$acceptance$No$share`).

Regarding H1, we find a consistent association between trust in science and acceptance of the scientific consensus (Studies 1: r = `r exp1_descriptives$cor_trust_acceptance$estimate`, p `r exp1_descriptives$cor_trust_acceptance$p.value`; 2: r = `r exp2_descriptives$cor_trust_acceptance$estimate`, p `r exp2_descriptives$cor_trust_acceptance$p.value`; 3: r = `r exp3_descriptives$cor_trust_acceptance$estimate`, p `r exp3_descriptives$cor_trust_acceptance$p.value`; 4: r = `r exp4_descriptives$cor_trust_acceptance$estimate`, p `r exp4_descriptives$cor_trust_acceptance$p.value`), but a less consistent relation between trust in science and science knowledge (Studies 1: r = `r exp1_descriptives$cor_trust_knowledge$estimate`, p `r exp1_descriptives$cor_trust_knowledge$p.value`; 2: r = `r exp2_descriptives$cor_trust_knowledge$estimate`, p `r exp2_descriptives$cor_trust_knowledge$p.value`; 3: r = `r exp3_descriptives$cor_trust_knowledge$estimate`, p `r exp3_descriptives$cor_trust_knowledge$p.value`; study 4: r = `r exp4_descriptives$cor_trust_knowledge$estimate`, p `r exp4_descriptives$cor_trust_knowledge$p.value`).

Regarding H2, the results are mixed both for the relation between conspiracy beliefs (measured as the average acceptance of all the conspiracy beliefs) and acceptance of the scientific consensus (Studies 1: r = `r exp1_descriptives$cor_conspiracy_acceptance$estimate`, p `r exp1_descriptives$cor_conspiracy_acceptance$p.value`; 2: r = `r exp2_descriptives$cor_conspiracy_acceptance$estimate`, p `r exp2_descriptives$cor_conspiracy_acceptance$p.value`; 3: r = `r exp3_descriptives$cor_conspiracy_acceptance$estimate`, p `r exp3_descriptives$cor_conspiracy_acceptance$p.value`; 4: r = `r exp4_descriptives$cor_conspiracy_acceptance$estimate`, p `r exp4_descriptives$cor_conspiracy_acceptance$p.value`) as well as science knowledge (Studies 1: r = `r exp1_descriptives$cor_conspiracy_knowledge$estimate`, p `r exp1_descriptives$cor_conspiracy_knowledge$p.value`; study 2: r = `r exp2_descriptives$cor_conspiracy_knowledge$estimate`, p `r exp2_descriptives$cor_conspiracy_knowledge$p.value`; 3: r = `r exp3_descriptives$cor_conspiracy_knowledge$estimate`, p `r exp3_descriptives$cor_conspiracy_knowledge$p.value`; 4: r = `r exp4_descriptives$cor_conspiracy_knowledge$estimate`, p `r exp4_descriptives$cor_conspiracy_knowledge$p.value`).

Why did participants reject the scientific consensus? We got a total of `r exp4_descriptives$justifications_n + exp3_descriptives$justifications_n + exp2_descriptives$justifications_n` answers (study 2: `r exp2_descriptives$justifications_n`; study 3: `r exp3_descriptives$justifications_n`; study 4: `r exp4_descriptives$justifications_n`) from `r exp4_descriptives$justifications_n_participants + exp3_descriptives$justifications_n_participants + exp2_descriptives$justifications_n_participants` (study 2: `r exp2_descriptives$justifications_n_participants`; study 3: `r exp3_descriptives$justifications_n_participants`; study 4: `r exp4_descriptives$justifications_n_participants`) different participants to the open-ended questions on why they had rejected the scientific consensus on a particular question. Based on the answers, we created five categories. Table \@ref(tab:justifications) summarizes these answers by five categories. All answers can be read in the OSM.

```{r justifications}
## make a common table of exp 2 and exp 3
justifications <- bind_rows(exp2_justifications %>% 
                              mutate(study = "study 2"), 
                            exp3_justifications %>% 
                              mutate(study = "study 3", 
                                     final_coding = as.character(final_coding)),
                            exp4_justifications %>% 
                              mutate(study = "study 4"))

justifications %>%
  group_by(category)%>%
  summarize(n = n(), 
            n_subjects = n_distinct(id)) %>%
  mutate(Share = n/sum(n)) %>%
  mutate_if(is.numeric, round, digits=3) %>% 
  mutate(Share = paste0(Share*100, "%")) %>% 
  arrange(desc(n)) %>% 
  select(category, n, Share, n_subjects) %>% 
  ungroup() %>% 
  rename(
    Category = category,
    `N (instances)` = n, 
    `Share (instances)` = Share, 
    `N (unique participants)` = n_subjects
  ) %>% 
  apa_table(caption = "Justifications by category for data from studies 2, 3 and 4 combined.")
```

Why did participants accept the scientific consensus? In Studies 3 and 4--the vaccine skeptic samples--we had asked participants about cases where they agreed with the scientific consensus. A total of `r exp3_descriptives$acceptance_n + exp4_descriptives$acceptance_n` (Study 3: `r exp3_descriptives$acceptance_n`; Study 4: `r exp4_descriptives$acceptance_n`) participants answered this question. There were more participants saying they accepted the scientific consensus because they independently verified the fact (Study 3: `r exp3_descriptives$acceptance_by_reason[["independent verification"]]$share`; Study 4: `r exp4_descriptives$acceptance_by_reason[["independent verification"]]$share` ), than there were participants saying it was because they trust scientists (Study 3: `r exp3_descriptives$acceptance_by_reason[["trust in scientists"]]$share`; Study 4: `r exp4_descriptives$acceptance_by_reason[["trust in scientists"]]$share`). We asked participants who said they verified independently to explain how they did it (answers can be found in the ESM). `r exp3_descriptives$acceptance_by_reason[["other"]]$share` in study 3 and `r exp4_descriptives$acceptance_by_reason[["other"]]$share` in study 4 answered with other "other" and gave an open-ended explanation (see ESM).

In an exploratory analysis, we ran linear regressions to test whether there are differences between these two groups. Perhaps unsurprisingly, those who said they accepted consensus because of trust in scientists reported to trust science more (Study 3: mean = `r exp3_descriptives$reason_means$'trust in scientists'$wgm_sciencegeneral_mean`; $\hat{\beta}_{\text{Trust}}$ = `r exp3_descriptives$reason_trust_model$'reason_agreementtrust in scientists'$estimate`, `r exp3_descriptives$reason_trust_model$'reason_agreementtrust in scientists'$p.value` on a scale from 1 to 4; Study 4: mean = `r exp4_descriptives$reason_means$'trust in scientists'$wgm_sciencegeneral_mean`; $\hat{\beta}_{\text{Trust}}$ = `r exp4_descriptives$reason_trust_model$'reason_agreementtrust in scientists'$estimate`, `r exp4_descriptives$reason_trust_model$'reason_agreementtrust in scientists'$p.value`) than those who said they verified independently (Study 3 mean = `r exp3_descriptives$reason_means$'independent verification'$wgm_sciencegeneral_mean`; Study 4 mean = `r exp4_descriptives$reason_means$'independent verification'$wgm_sciencegeneral_mean`). We did not find a difference regarding acceptance (Study 3: $\hat{\beta}_{\text{Acceptance}}$ = `r exp3_descriptives$reason_acceptance_model$'reason_agreementtrust in scientists'$estimate`, `r exp3_descriptives$reason_acceptance_model$'reason_agreementtrust in scientists'$p.value` on a scale from 0 to 1; Study 4: $\hat{\beta}_{\text{Acceptance}}$ = `r exp4_descriptives$reason_acceptance_model$'reason_agreementtrust in scientists'$estimate`, `r exp4_descriptives$reason_acceptance_model$'reason_agreementtrust in scientists'$p.value`). In study 4, where we used facts that participants were unlikely to have encountered before, we tracked whether people clicked on the source links we provided--a behavior that you would expect from people who report verifying facts independently. On average, participants clicked only clicked on `r exp4_descriptives$reason_means$'trust in scientists'$sum_link_clicks_mean` links (out of 20 possible clicks) and there was no difference between the two groups ($\hat{\beta}_{\text{Clicks}}$ = `r exp4_descriptives$reason_clicks_model$'reason_agreementtrust in scientists'$estimate`, `r exp4_descriptives$reason_clicks_model$'reason_agreementtrust in scientists'$p.value`).

More detailed results addressing all our pre-registered research questions can be found in the OSM.

# Discussion

Survey data from the US suggests a decline of trust in science in recent years [@brianAmericansTrustScientists2023]. Across the globe, many people only trust science ‘some of the time’ , and a sizable minority doesn’t trust it [@wellcomeglobalmonitorWellcomeGlobalMonitor2018; @wellcomeglobalmonitorWellcomeGlobalMonitor2020]. This appears difficult to reconcile with the fact that, in many countries, most people nowadays receive at least a basic education [@ritchieGlobalEducation2023], and that science education has been found to be the most strongly associated variable with trust in science [@wellcomeglobalmonitorWellcomeGlobalMonitor2018].

But how deep is distrust of science? In four studies, we have shown that almost every participant accepts almost all of basic science questions we included. Even when people do not know the correct answer to a science question, they tend to mostly accept the scientific consensus afterwards. That is true not only for basic science questions (studies 1-3), but also for more recent scientific findings that most people are unfamiliar with (study 4). In study 1 the acceptance rate after having given a wrong answer was considerably lower (`r exp1_descriptives$conditional_acceptance$false$No$share`) than in subsequent studies (study 2: `r exp2_descriptives$conditional_acceptance$false$Yes$share`; study 3: `r exp3_descriptives$conditional_acceptance$false$acceptance$Yes$share`; study 4: `r exp4_descriptives$conditional_acceptance$false$acceptance$Yes$share`). While this could be just sampling variation, it might be that adding explanations and/or sources convinced participants more than merely stating the consensus. In general, participants with lower trust in science and who believe more in conspiracy theories tended to both know less about science and accept the scientific consensus less.

Scholars have long interpreted the lack of basic science knowledge as the root cause of distrust in and more generally unfavorable attitudes towards science, an explanation known under the term "deficit-model" [@millerMeasurementCivicScientific1998a; @millerPublicUnderstandingAttitudes2004; @durantPublicUnderstandingScience1989]. In line with proponents of the deficit-model, we find that people indeed err even on very basic science questions. However, we also find that almost everyone accepted the scientific consensus once informed about it. Moreover, the same was true for non-basic science questions. This calls into question the deficit model: If participants trust science on basic (and non-basic) questions—as suggests immediate acceptance of the scientific consensus—then unfavorable attitudes towards science must have other explanations than a mere lack of knowledge. This argument is supported by past research. In a seminal meta-analysis, \@allumScienceKnowledgeAttitudes2008 found correlations between science knowledge and general attitudes towards science to be at best small and largely context dependent [\@allumScienceKnowledgeAttitudes2008]. A more recent report by the National Academies of Sciences, Engineering, and Medicine states that science knowledge is "unlikely to substantially affect attitudes on scientific issues" [@nationalacademiesofsciencesCommunicatingScienceEffectively2017].

How to make sense of the detachment of basic science knowledge from attitudes towards science? One explanation might be that most scientific findings are very abstract. For example, to most people, it will not be relevant for their daily lives to know that the speed of light is 299'792'458 meters per second. It is therefore easy to simply accept it, all the more if it appears to be consensual within the population of scientists. In other words, people might hold on to most scientific findings as reflective beliefs—beliefs that have little effect on our behaviors [@mercierHowGoodAre2021]. It is probably when it becomes tangible that science shape's people's attitudes. This can happen, for example, when science appears to support conclusions in favor of certain policies or moral positions, as for example in the case of anthropogenic climate change or vaccines.

It is tempting, especially for scientists, to explain mistrust in science with ignorance. After all, careful proof of objective truth is what makes science trustworthy—that is, for scientists. For most people, what shapes trust in science might not be so different from what shapes trust in other institutions, such as the government, banks or church.

\FloatBarrier

# References

::: {#refs}
:::

\newpage

# (APPENDIX) Appendix {.unnumbered}

```{r child = "appendix_exp1.Rmd"}
```

\clearpage

```{r child = "appendix_exp2.Rmd"}
```

\clearpage

```{r child = "appendix_exp3.Rmd"}
```

\clearpage

```{r child = "appendix_exp4.Rmd"}
```
