---
title             : "Universal acceptance of (nearly all of) basic science in the US"
shorttitle        : "Acceptance of basic science"

header-includes:  | # to prevent floats from moving past certain points (for the appendix)
  \usepackage{placeins} 
  
author: 
  - name          : ""
    affiliation   : ""

affiliation:
  - id            : ""
    institution   : ""
    
abstract: |
  XX
  
keywords          : 



floatsintext      : yes
linenumbers       : no 
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "doc" # "doc" for nice look, "man" for manuscripty
output            : papaja::apa6_pdf # "_doc" for word; however, note that some of the features of kableExtra are not available for word and will yield errors. For example to knit to word, you'll have to comment out all "add_header_above()" functions for model output tables, or e.g. set always allow html as true in the yaml heading

always_allow_html: true

appendix:
  - "appendix_exp1.Rmd"
  - "appendix_exp2.Rmd"
  - "appendix_exp3.Rmd"

bibliography: references.bib
---

```{r setup, include=FALSE}
# Figure out output format
is_docx <- knitr::pandoc_to("docx") | knitr::pandoc_to("odt")
is_latex <- knitr::pandoc_to("latex")
is_html <- knitr::pandoc_to("html")

# Word-specific things
table_format <- ifelse(is_docx, "huxtable", "kableExtra")  # Huxtable tables
conditional_dpi <- ifelse(is_docx, 300, 300)  # Higher DPI
conditional_align <- ifelse(is_docx, "default", "center")  # Word doesn't support align

# Knitr options
knitr::opts_chunk$set(
  echo = FALSE, warning = FALSE, message = FALSE,
  # tidy.opts = list(width.cutoff = 120),  # Code width
  # fig.retina = 3, dpi = conditional_dpi,
  # fig.width = 7, fig.asp = 0.618,
  # fig.align = conditional_align, out.width = "100%",
  fig.path = "output/figures/",
  cache.path = "output/_cache/",
  fig.process = function(x) {  # Remove "-1" from figure names
    x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
    if (file.rename(x, x2)) x2 else x
  },
  options(scipen = 99999999)  # Prevent scientific notation
)

# R options
options(
  width = 90,  # Output width
  dplyr.summarise.inform = FALSE,  # Turn off dplyr's summarize() auto messages
  knitr.kable.NA = "",  # Make NAs blank in kables
  kableExtra.latex.load_packages = FALSE,  # Don't add LaTeX preamble stuff
  modelsummary_factory_default = table_format,  # Set modelsummary backend
  modelsummary_format_numeric_latex = "plain"  # Don't use siunitx
)
```

```{r packages, include=FALSE}
# load required packages
library("papaja")      # For APA style manuscript   
library("lme4")        # model specification / estimation
library("lmerTest")    # provides p-values in the output
library("tidyverse")   # data wrangling and visualisation
library("afex")        # anova and deriving p-values from lmer
library("broom")       # extracting data from model fits 
library("broom.mixed") # extracting data from mixed models
library("metafor")     # doing mata analysis
library("patchwork")   # put several plots together
library("ggridges")    # for plots
library("gghalves")    # for plots
library("ggbeeswarm")  # Special distribution-shaped point jittering
library("knitr")       # for tables
library("kableExtra")  # also for tables
library("ggpubr")      # for combining plots with ggarrange() 
library("grid")        # for image plots   
library("gridExtra")   # for image plots
library("png")         # for image plots
library("modelsummary") # for regression tables
library("ggrepel") # for better random patterns in plots
```

```{r functions}
# load plot theme
source("functions/plot_theme.R") 

# load other functions
source("functions/own_functions.R")
```

# Introduction

How much do people trust and accept science? The recent COVID pandemic has highlighted the importance of this question, with trust in science being the best predictor of people’s proclivity to follow health guidelines [@alganTrustScientistsTimes2021].

Unfortunately, trust in science is far from being at ceiling [@wellcomeglobalmonitorWellcomeGlobalMonitor2018; @wellcomeglobalmonitorWellcomeGlobalMonitor2020], and there are suggestions that it has recently been dropping in some countries [@alganTrustScientistsTimes2021] and that it is increasingly politicized in the US [@gauchatPoliticizationSciencePublic2012; @krauseTrendsAmericansTrust2019a; @liPolarizationPublicTrust2022].

This relative lack of trust in science could mean different things. It could mean that some people reject science wholesale, or at least to a significant extent. Or it could mean that they maintain a high degree of trust in most of science, and only question specific results that happen to contradict specific other beliefs they have (creationism, etc.), or beliefs that are associated with disliked behaviors (vaccination, acting for climate change).

We answer this question by looking at how much Americans accept basic scientific knowledge. We’ll pay special attention to two groups of participants: people who say they don’t trust science (much, or at all), and people who endorse science-related conspiracy theories (CTs), or have a CT mindset, since this has been linked with a low degree of trust in science, and that many CTs imply a mistrust of at least some science (vaccination, hydroxychloroquine, etc.).

We presently briefly review work on trust in science, and science knowledge, before turning to work on CT and trust in science.

## Trust in science

Across the globe, most people say they trust science at least to some extent. In 2018, the Wellcome Global Monitor (WGM) surveyed of over 140'000 people in over 140 countries on trust in science [@wellcomeglobalmonitorWellcomeGlobalMonitor2018] . In 2020, during the first year of the Covid pandemic, a follow up survey was made in 113 countries, involving 119'000 participants [@wellcomeglobalmonitorWellcomeGlobalMonitor2020]. Across all countries, in 2018, 32% of participants said they trust science "a lot" (41% in 2020), 45% trust "some" science (39% in 2020), and only 13% percent trust science "not much/ not at all" (13% in 2020), while 10% indicated "don't know" (7% in 2020) (<https://wellcome.org/reports/wellcome-global-monitor-covid-19/2020#gid=6acd&pid=0>).

Trust in science also appears to be relatively stable in the US [@funkPublicConfidenceScientists2020]. However, there are some potentially worrying trends: The US might be increasingly polarized on trust in science [@gauchatPoliticizationSciencePublic2012]. For example, the partisan divide on the question of whether climate change is a major threat to societey [has been growing in the last decade](https://www.pewresearch.org/short-reads/2023/08/09/what-the-data-says-about-americans-views-of-climate-change/) [@kennedyWhatDataSays2023]. And while by some measures only about half of the US population believes in anthropogenic climate change, people do mostly not seem to change their mind on on the issue [@mottaChangingMindsChanging2021].

Trust in science is typically measured in three ways: either asking explictly about general trust in science or scientists, by asking about science attitudes (e.g. XX), or asking questions about specific, typically contentious, science topics (e.g. vaccines, climate change, GMOs). It is unclear to which extent these measures capture trust in basic science. To assess this, we can look at the literature on science knowledge.

## Science knowledge

A lot of research has been conducted on the public understanding of science. Part of this research are surveys measuring to which extent people have basic science knowledge (e.g. "Does the Earth go around the Sun, or does the Sun go around the Earth?"). For us, these surveys provide a lower bound to how much people trust basic science: people who give the right answer (and not by chance) supposedly trust science on that question. Unfortunately, that lower bound has often been indeed pretty low: researchers have pointed out the deficit in basic science knowledge [@millerPublicUnderstandingAttitudes2004; @millerMeasurementCivicScientific1998a]. For example, only 55% in the US (in 2014) and only 46% in the EU (in 2005) knew that antibiotics kill only bacteria but not viruses [@committeeonscienceliteracyandpublicperceptionofscienceScienceLiteracyConcepts2016]. A question that remains is whether people do not provide the right answers because they are not familiar with the scientific consensus, or because they explicitly reject that consensus.

[To do: Make a descriptive overview of science knowledge across time/countries. This is a crucial point: if there are questions with specific scientific answers that just about everyone already knows, then in a sense our point is already proven. But there are enough questions where people's knowledge appears quite poor anyways]

## CT and TiS

Recently, the relationship of one group to trust in science (TiS) has been particularly scrutinized: conspiracy theorists (CTists), either defined as people who endorse specific conspiracy theories, or as people who have a conspiracy mindset [@rutjensConspiracyBeliefsScience2022; @vranicDidMyOwn2022]. CTists reject specific science knowledge (related to their CT), and they tend to trust science less on the whole. What remains unclear is to which extent CTists (dis)trust basic science.

## The present studies

In a series of studies, we ask basic science knowledge questions, inform people of the scientific consensus, and see if they accept it. Of particular interest are people who are low in TiS, and CTists.

In study 1, we plainly inform participants of the scientific consensus and ask if they accept it. We use a subset of questions on basic science knowledge that have been used in numerous public opinion surveys [@allumScienceKnowledgeAttitudes2008; @durantPublicUnderstandingScience1989; @millerMeasurementCivicScientific1998a] sometimes referred to as the "Oxford scale" [@gauchatCulturalAuthorityScience2011]. In study 2, we remove a problematic science question and present people with a more elaborated explanation of the scientific consensus, as well as additional sources. This allows us to remove the issue that participants might simply not have trusted us to correctly report the scientific consensus in study 1. Study 3 follows essentially the same design as study 2, but is run on a vaccine-skeptic sample. Further, we provided participants with an explicit option to revise their answer, in case they rejected the consensus. We also asked follow-up questions on why participants accepted the scientific consensus, namely whether they thought it was because they trust scientists or because they verified independently. Study 4 uses the same design as study 3, but asking science questions that people are less likely to have encountered before, and that they would be less likely to be able to verify (or even understand) themselves [on an anti-vaxx sample again?].

Then: replications in other countries?

# Overview of experiments

The goal is to see to which extent people do not accept basic science.

# Experiment 1

```{r exp1}
# Analyze data of experiments and store results

# Experiment 1

# read the cleaned long version data set
exp1_long <- read_csv("exp_1/data/cleaned_long.csv")

# read wide version data set
exp1_wide <- read_csv("exp_1/data/cleaned_wide.csv") 

# Research questions

# RQ3
cor_trust_knowledge <- cor.test(exp1_wide$wgm_sciencegeneral, exp1_wide$avg_knowledge) %>% text_ready()

cor_trust_acceptance <- cor.test(exp1_wide$wgm_sciencegeneral, exp1_wide$avg_acceptance) %>% text_ready()

# RQ4
cor_conspiracy_knowledge <- cor.test(exp1_wide$BCTI_avg, exp1_wide$avg_knowledge) %>% text_ready()

cor_conspiracy_acceptance <- cor.test(exp1_wide$BCTI_avg, exp1_wide$avg_acceptance)%>% text_ready()

# Exploratory: Acceptance and trust in science/conspiracy thinking conditional on false answers only
exp1_false_knowledge <- exp1_long %>% 
  # make numeric versions
  mutate(acceptance_num = ifelse(acceptance == "Yes", 1, 0)
  ) %>% 
  group_by(id, knowledge) %>% 
  # calculate by-participant averages
  summarize(
    n = n(),
    n_accepted = sum(acceptance_num),
    avg_acceptance  = sum(acceptance_num)/n(), 
    wgm_sciencegeneral = mean(wgm_sciencegeneral), 
    BCTI_avg = mean(BCTI_avg), 
    # add additonal measures for robustness checks
    CMQ_avg = mean(CMQ_avg), 
    SICBS = mean(SICBS),
    wgm_scientists = mean(wgm_scientists), 
    pew = mean(pew)
  ) %>% 
  # filter to only false responses
  filter(knowledge == FALSE) %>% 
  ungroup()

false_answers_cor_trust_acceptance <- cor.test(exp1_false_knowledge$wgm_sciencegeneral, exp1_false_knowledge$avg_acceptance) %>% text_ready()

false_answers_cor_conspiracy_acceptance <- cor.test(exp1_false_knowledge$BCTI_avg, exp1_false_knowledge$avg_acceptance) %>% text_ready()


# extract descriptives for inline reporting
exp1_descriptives <- list(
  # Demographics
  n_subj = n_distinct(exp1_wide$id),
  gender = exp1_wide %>% group_by(gender) %>% summarize(n = n_distinct(id)) %>% split(.$gender),
  age = exp1_wide %>% summarize(across(age,list(mean = mean, median = median, sd = sd), 
                                       .names = "{.fn}")) %>% rounded_numbers(),
  # RQ1 & RQ2
  means = exp1_wide %>% 
    summarize(across(c(avg_knowledge, avg_acceptance), 
                     list(mean = mean, sd = sd), 
                     .names = "{.col}_{.fn}")) %>%
    rename_with(~sub("^avg_", "", .), everything()) %>% 
    mutate_if(is.numeric, round, digits = 2) %>% 
  mutate(across(ends_with("_mean"), ~paste0(.*100, " %"))) ,
  # RQ3
  cor_trust_knowledge = cor_trust_knowledge,
  cor_trust_acceptance = cor_trust_acceptance,
  # RQ4
  cor_conspiracy_knowledge = cor_conspiracy_knowledge,
  cor_conspiracy_acceptance = cor_conspiracy_acceptance, 
  # Exploratory
  # acceptance and knowledge
  conditional_acceptance = exp1_long %>% 
    group_by(knowledge, acceptance) %>% 
    count() %>% 
    group_by(knowledge) %>% 
    mutate (share = n/sum(n), 
            # rename knowledge values
            knowledge = ifelse(knowledge == FALSE, "false", "true")) %>% 
    mutate_if(is.numeric, round, digits = 3) %>% 
    mutate (share = paste0(share*100, " %")) %>% 
    super_split(knowledge, acceptance),
  # acceptance and trust in science/conspiracy thinking
  false_answers_cor_trust_acceptance = false_answers_cor_trust_acceptance,
  false_answers_cor_conspiracy_acceptance = false_answers_cor_conspiracy_acceptance
)
```

The main goal of experiment one was to test whether people would accept the scientific consensus on basic knowledge questions. Additionally, we wanted to know if both science knowledge and acceptance of the scientific consensus are associated with trust in science and conspiracy thinking. We had the following research questions:

**RQ1: What is the average science knowledge score (1)?**

**RQ2: What is the average acceptance of the scientific consensus (2)?**

**RQ3: What is the relationship between trust in science and, respectively, (1) and (2)?**

**RQ4: What is the relationship between conspiracy thinking and, respectively, (1) and (2)?**

## Methods

### Participants

We recruited 200 participants from the US via prolific. 6 participants failed our attention check, resulting in a final sample of `r exp1_descriptives$n_subj` participants (`r exp1_descriptives$gender$female$n` female, `r exp1_descriptives$gender$male$n` male; $age_\text{mean}$: `r exp1_descriptives$age$mean`, $age_\text{sd}$: `r exp1_descriptives$age$sd`, $age_\text{median}$: `r exp1_descriptives$age$median`). Since we did not have any prior assumptions on effect sizes, we did not do a power analysis.

### Procedure

After providing their consent to participate in the study, participants were given an attention check "While watching the television, have you ever had a fatal heart attack?" [1-6; 1 = Never, 6 = Often]. All participants who did not answer "1 = Never" were excluded. Participants then read the following instructions:"We will ask you 11 questions about science. After each question, we will provide you with the scientifically consensual answer and ask whether you accept it." Next, participants answered a set of 10 basic science questions, which were randomly selected from a pool of 11 questions, in random order. After each question, participants were presented with an answer reflecting the scientific consensus. Participants were asked to choose whether they accept the answer or not, before proceeding to the next question. Figure \@ref(fig:stimulus-example) displays the survey for an example science question. Finally, participants answered questions on conspiracy thinking and trust in science.

### Materials

#### Science knowledge and acceptance

Table \@ref(tab:knowledge) shows all questions, their scientifically consensual answer, and their source. All but two questions were selected from existing science knowledge questionnaires. We tried to select non-political questions.

```{r knowledge}
# Function to replace special characters in all cells of a data frame
replace_special_characters <- function(df) {
  df %>%
    mutate(across(everything(), ~ gsub("\u2013|\u2014|\u2212", "-", .))) %>% # Replace en-dash, em-dash, and Unicode minus with ASCII hyphen-minus
    mutate(across(everything(), ~ iconv(., from = "UTF-8", to = "ASCII//TRANSLIT"))) # Ensure all text is ASCII
}

# Read the CSV file
items <- read_csv("exp_4/materials/overview_questions.csv")

# Replace special characters
items_cleaned <- replace_special_characters(items) %>% 
  mutate(id = 1:nrow(.)) %>%
  select(id, everything())  # Ensure 'id' is the first column

# add footnote for the tree items
items_cleaned[11, 2] <- paste0("*", 
                               items_cleaned[11, 2])
  

# Output the table
knitr::kable(items_cleaned, booktabs = T, longtable = TRUE,
    caption = "Science knowledge items", 
    full_width = T, 
    col.names = c("", "Study 1-3", "Study 4")) %>%
  kable_styling(font_size = 8) %>%
  column_spec(1, width = "2em") %>%
  column_spec(2, width = "22em")%>%
  column_spec(3, width = "22em") %>% 
  footnote(symbol = "Only used in Study 1")
```

#### Conspiracy scales

To measure conspiracy thinking, we selected 10 science/health related conspiracy theories from the Belief in Conspiracy Theory Inventory (BCTI) by @pennycookOverconfidentlyConspiratorialConspiracy2022 (Table \@ref(tab:conspiracy)). Participants were asked: "Below is a list of events for which the official version has been disputed. For each event, we would like you to indicate to what extent you believe the cover-up version of events is true or false. [1-9; labels: 1 - completly false, 5 - unsure, 9 - completely true]". 

```{r conspiracy, echo=FALSE}
# Create the data frame
items <- c(
  "The Apollo moon landings never happened and were staged in a Hollywood film studio.",
  "A cure for cancer was discovered years ago, but this has been suppressed by the pharmaceutical industry and the U.S. Food and Drug Administration (FDA).",
  "The spread of certain viruses and/or diseases is the result of the deliberate, concealed efforts of vested interests.",
  "The claim that the climate is changing due to emissions from fossil fuels is a hoax perpetrated by corrupt scientists who want to spend more taxpayer money on climate research.",
  "The Earth is flat (not spherical) and this fact has been covered up by scientists and vested interests.",
  "There is a causal link between vaccination and autism that has been covered up by the pharmaceutical industry.",
  "In the 1950s and 1960s more than 100 million Americans received a polio vaccine contaminated with a potentially cancer-causing virus.",
  "Proof of alien contact is being concealed from the public.",
  "Hydroxychloroquine has been demonstrated to be a safe and effective treatment of COVID and this information is being suppressed.",
  "Dinosaurs never existed, evolution is not real, and scientists have been faking the fossil record.")

conspiracy_items <- data.frame(id = 1:length(items), items = items)

# Output the table
kbl(conspiracy_items, booktabs = T, longtable = T, col.names = NULL, 
    caption = "Conspiracy items", 
    full_width = F) %>%
  kable_styling(font_size = 8) %>% 
  column_spec(1) %>%
  column_spec(2, width = "40em")
```

To cross-check our results with alternative measures, we also assessed the conspiracy mentality questionnaire (CMQ) by @bruderMeasuringIndividualDifferences2013 and the Single Item Conspiracy Beliefs Scale (SICBS) by @lantianMeasuringBeliefConspiracy2016 (see Appendix \@ref(exp1)).

#### Trust in science

Our main item for measuring trust in science is selected from the Wellcome Global Monitor survey: "In general, would you say that you trust science a lot, some, not much, or not at all? [1 = Not at all, 2 = Not much, 3 = Some, 4 = A lot]"

We also included two additional trust questions, one also from the Wellcome Global Monitor (WGM) survey ("How much do you trust scientists in this country? Do you trust them a lot, some, not much, or not at all? [1 = Not at all, 2 = Not much, 3 = Some, 4 = A lot]"), the other from the Pew research center ("How much confidence do you have in scientists to act in the best interests of the public? [1-5; 1 = No confidence at all, 5 = A great deal of confidence]"). We selected these items so that we could compare the ratings in our sample to global survey results. The WGM survey has been administered in over 140 countries and included over 140000 respondents. The Pew question has recently been used by a world-wide many labs study in 67 countries with 71417 respondents [@colognaTrustScientistsTheir2024].

## Results

Regarding RQ1 and RQ2, participants answered on average `r exp1_descriptives$means$knowledge_mean` (sd = `r exp1_descriptives$means$knowledge_sd`) of the questions correctly, and accepted the scientific consensus on average for `r exp1_descriptives$means$acceptance_mean` (sd = `r exp1_descriptives$means$acceptance_sd`) of the questions.

Fig. \@ref(fig:exp1-conditional-acceptance) illustrates the relationship between knowledge and acceptance. In most cases (`r exp1_descriptives$conditional_acceptance$false$Yes$share`), participants readily accepted the scientific consensus after having given the wrong answer to a question. In very few cases (`r exp1_descriptives$conditional_acceptance$true$No$share`), participants who gave the correct response afterwards rejected the scientific consensus, thereby contradicting their own initial response. We believe this might have been due to inattention.

For RQ3, we find a positive but small correlation between both science knowledge and trust in science (r = `r exp1_descriptives$cor_trust_knowledge$estimate`, p `r exp1_descriptives$cor_trust_knowledge$p.value`), and acceptance of scientific consensus and trust in science (r = `r exp1_descriptives$cor_trust_acceptance$estimate`, p `r exp1_descriptives$cor_trust_acceptance$p.value`). The more people are knowledgeable about science and the more they tend to accept the scientific consensus, the more they tend to trust science. These correlations are relatively weak, which might be partly due to ceiling effects: As illustrated in Fig. \@ref(fig:exp1-plot), (i) most people do trust science, and (ii) that is true even among people with low knowledge or acceptance rates.

For RQ4, we find a negative correlation of similar magnitude between conspiracy thinking and science knowledge (r = `r exp1_descriptives$cor_conspiracy_knowledge$estimate`, p `r exp1_descriptives$cor_conspiracy_knowledge$p.value`), and conspiracy thinking and acceptance of scientific consensus (r = `r exp1_descriptives$cor_conspiracy_acceptance$estimate`, p `r exp1_descriptives$cor_conspiracy_acceptance$p.value`).

In Appendix \@ref(exp1), we show that these associations are robust when using alternative measures of trust and conspiracy thinking. Appendix \@ref(exp1) also includes more descriptive statistics, such as knowledge and acceptance by science questions.

Are trust in science and conspiracy thinking, respectively, associated with being more easily convinced of the scientific consensus? In our main analyses, we looked at correlations of acceptance across all observations. One possibility is that the associations between trust in science/conspiracy thinking and acceptance of scientific consensus are explained by science knowledge: People who give the right answer in the first place are more ready to accept the consensus, and trust in science/conspiracy thinking are mostly associated with this knowledge, but not with willingness to accept the consensus. To addressed this potential confound, in a non-preregistered analysis, we restricted our sample to cases where participants gave the wrong answer to the knowledge question. We than calculated the correlation between trust in science and average acceptance rate by participant. We find no statistically significant correlation of acceptance with neither conspiracy thinking (r = `r exp1_descriptives$false_answers_cor_conspiracy_acceptance$estimate`, p `r exp1_descriptives$false_answers_cor_conspiracy_acceptance$p.value`), nor with trust in science (r = `r exp1_descriptives$false_answers_cor_trust_acceptance$estimate`, p `r exp1_descriptives$false_answers_cor_trust_acceptance$p.value`).

## Discussion

These results suggest that most people accept the scientific consensus most of the time. Even when people do not know the correct answer to a science question, they tend to mostly accept the scientific consensus afterwards. Yet, in `r exp1_descriptives$conditional_acceptance$false$No$share` of the cases, participants rejected the scientific consensus after having given the wrong answer, suggesting that simply stating the consensus is not sufficient to convince participants sometimes. In general, people with lower trust in science and who believe more in conspiracy theories tend to both know less about science and accept the scientific consensus less.

# Experiment 2

```{r exp2}
# Analyze data of experiments and store results

# Experiment 2

# read the cleaned long version data set
exp2_long <- read_csv("exp_2/data/cleaned_long.csv")

# read wide version data set
exp2_wide <- read_csv("exp_2/data/cleaned_wide.csv") 

# read coded justifications
exp2_justifications <- read_csv("exp_2/data/justifications_clean.csv")

# Research questions

# H1a
cor_trust_knowledge <- cor.test(exp2_wide$wgm_sciencegeneral, exp2_wide$avg_knowledge) %>% text_ready()

# H2a
cor_conspiracy_knowledge <- cor.test(exp2_wide$BCTI_avg, exp2_wide$avg_knowledge) %>% text_ready()

# Exploratory: Acceptance and trust in science/conspiracy thinking conditional on false answers only
exp2_false_knowledge <- exp2_long %>% 
  # make numeric versions
  mutate(acceptance_num = ifelse(acceptance == "Yes", 1, 0)
  ) %>% 
  group_by(id, knowledge) %>% 
  # calculate by-participant averages
  summarize(
    n = n(),
    n_accepted = sum(acceptance_num),
    avg_acceptance  = sum(acceptance_num)/n(), 
    wgm_sciencegeneral = mean(wgm_sciencegeneral), 
    BCTI_avg = mean(BCTI_avg), 
    # add additonal measures for robustness checks
    CMQ_avg = mean(CMQ_avg), 
    SICBS = mean(SICBS),
    wgm_scientists = mean(wgm_scientists), 
    pew = mean(pew)
  ) %>% 
  # filter to only false responses
  filter(knowledge == FALSE) %>% 
  ungroup()

# H1b
# conditional on false answers
false_answers_cor_trust_acceptance <- cor.test(exp2_false_knowledge$wgm_sciencegeneral, exp2_false_knowledge$avg_acceptance) %>% text_ready()

# pooled across all cases
cor_trust_acceptance <- cor.test(exp2_wide$wgm_sciencegeneral, exp2_wide$avg_acceptance) %>% text_ready()

# H2b
# conditional on false answers
false_answers_cor_conspiracy_acceptance <- cor.test(exp2_false_knowledge$BCTI_avg, exp2_false_knowledge$avg_acceptance) %>% text_ready()

# pooled across all cases
cor_conspiracy_acceptance <- cor.test(exp2_wide$BCTI_avg, exp2_wide$avg_acceptance) %>% text_ready()


# extract descriptives for inline reporting
exp2_descriptives <- list(
  # Demographics
  n_subj = n_distinct(exp2_wide$id),
  gender = exp2_wide %>% group_by(gender) %>% summarize(n = n_distinct(id)) %>% split(.$gender),
  age = exp2_wide %>% summarize(across(age,list(mean = mean, median = median, sd = sd), 
                                       .names = "{.fn}")) %>% rounded_numbers(),
  # RQ1 & RQ2
  means = exp2_wide %>% 
    summarize(across(c(avg_knowledge, avg_acceptance), 
                     list(mean = mean, sd = sd), 
                     .names = "{.col}_{.fn}")) %>%
    rename_with(~sub("^avg_", "", .), everything()) %>% 
    mutate_if(is.numeric, round, digits = 2) %>% 
  mutate(across(ends_with("_mean"), ~paste0(.*100, " %"))) ,
  # H1
  # a
  cor_trust_knowledge = cor_trust_knowledge,
  # b (conditional on false responses)
  false_answers_cor_trust_acceptance = false_answers_cor_trust_acceptance,
  # b (all cases)
  cor_trust_acceptance = cor_trust_acceptance,
  # H2
  # a
  cor_conspiracy_knowledge = cor_conspiracy_knowledge,
  # b (conditional on false responses)
  false_answers_cor_conspiracy_acceptance = false_answers_cor_conspiracy_acceptance,
  # b(all cases)
  cor_conspiracy_acceptance = cor_conspiracy_acceptance, 
  # Exploratory
  # acceptance and knowledge
  conditional_acceptance = exp2_long %>% 
    group_by(knowledge, acceptance) %>% 
    count() %>% 
    group_by(knowledge) %>% 
    mutate (share = n/sum(n), 
            # rename knowledge values
            knowledge = ifelse(knowledge == FALSE, "false", "true")) %>% 
    mutate_if(is.numeric, round, digits = 3) %>% 
    mutate (share = paste0(share*100, " %")) %>% 
    super_split(knowledge, acceptance),
  # Justifications
  justifications_n = nrow(exp2_justifications), 
  justifications_n_participants = n_distinct(exp2_justifications$id),
  justification_by_category = exp2_justifications %>%
    group_by(category)%>%
    summarize(n = n(), 
              n_subjects = n_distinct(id)) %>%
    mutate(share = n/sum(n)) %>%
    mutate_if(is.numeric, round, digits=3) %>% 
    mutate(share = paste0(share*100, "%")) %>% 
    split(.$category)
)
```

In experiment 1, we tested whether participants would accept the scientific consensus on basic science facts. In most instances they did, but not always. In experiment 2, we wanted to test whether this reluctance was because of participants not trusting us as a source of consensual science knowledge. To do so, we added an explanation and sources to each consensus statement, instead of only stating the consensus. To better understand reasons for consensus rejection, after having answered all questions, we asked participants an open-ended question to explain why they rejected the consensus, for each question on which they did so. We also excluded the where do trees their materials from, as this question clearly seemed to be an outlier where most participants would get the answer wrong (see Appendix \@ref(exp1)).

Based on experiment 1, we formulated the following hypotheses:

**H1a: Higher trust in science is associated with more science knowledge?**

**H1b: Higher trust in science is associated with more acceptance of the scientific consensus, for participants who did not already know it?**

**H2a: Higher conspiracy thinking is associated with less science knowledge?**

**H2b: Higher conspiracy thinking is associated with less acceptance of the scientific consensus, for participants who did not already know it?**

We had the following research questions:

**RQ1: What is the average science knowledge score?**

**RQ2: When a participant’s answer does not match the consensus, how often do they change their mind and accept the consensus?**

**RQ3: What reasons do participants provide to justify their rejection of the scientific consensus?**

## Methods

### Participants

We recruited 201 participants from the US via prolific. 11 participants failed our attention check, resulting in a final sample of `r exp2_descriptives$n_subj` participants (`r exp2_descriptives$gender$female$n` female, `r exp2_descriptives$gender$male$n` male; $age_\text{mean}$: `r exp2_descriptives$age$mean`, $age_\text{sd}$: `r exp2_descriptives$age$sd`, $age_\text{median}$: `r exp2_descriptives$age$median`). Since we did not have any prior assumptions on effect sizes and our analyses were descriptive, we did not do a power analysis.

### Procedure

The procedure was the same as in experiment 1, with the difference that, instead of just stating the scientific consensus, participants were presented with a short explanation which we wrote, partly based on explanations generated by ChatGPT, and three links to authoritative sources supporting the answer (Fig. \@ref(fig:exp2-stimulus-example).

### Materials

We relied on the same items as in experiment 1. The only difference was that we removed the question on trees.

## Results

As in experiment 1, we find a positive but small correlation between science knowledge and trust in science (H1a: r = `r exp2_descriptives$cor_trust_knowledge$estimate`, p `r exp2_descriptives$cor_trust_knowledge$p.value`) and a small negative correlation between science knowledge and conspiracy thinking (H2a: r = `r exp2_descriptives$cor_conspiracy_knowledge$estimate`, p `r exp2_descriptives$cor_conspiracy_knowledge$p.value`). By contrast to experiment 1, we conditioned on initially false answers when looking at the relationship of consensus acceptance with trust in science and conspiracy thinking, respectively. For trust in science, we find no statistically significant correlation (r = `r exp2_descriptives$false_answers_cor_trust_acceptance$estimate`, p `r exp2_descriptives$false_answers_cor_trust_acceptance$p.value`). For conspiracy thinking we find a small negative one (r = `r exp2_descriptives$false_answers_cor_conspiracy_acceptance$estimate`, p `r exp2_descriptives$false_answers_cor_conspiracy_acceptance$p.value`).


Confirming results from experiment 1, we find that the more people are knowledgeable about science and the more they tend to accept the scientific consensus even when they are not that knowledgeable in science, the more they tend to trust science. These correlations are relatively weak, which might be partly due to ceiling effects: As illustrated in Fig. \@ref(fig:exp2-plot), (i) most people do trust science, and (ii) that is true even among people with low knowledge or acceptance rates. In Appendix \@ref(exp2) we show that these results hold for our alternative measures of trust and conspiracy thinking. We also include more descriptive statistics, such as knowledge and acceptance by science questions.

Regarding RQ1, participants answered on average `r exp2_descriptives$means$knowledge_mean` (sd = `r exp2_descriptives$means$knowledge_sd`) of the questions correctly, and accepted the scientific consensus on average for `r exp2_descriptives$means$acceptance_mean` (sd = `r exp2_descriptives$means$acceptance_sd`) of the questions. Fig. \@ref(fig:exp2-conditional-acceptance) illustrates the relationship between knowledge and acceptance. In response to RQ2, in most cases (`r exp2_descriptives$conditional_acceptance$false$Yes$share`), participants readily accepted the scientific consensus after having initially given the wrong answer to a question. In very few cases (`r exp2_descriptives$conditional_acceptance$true$No$share`), participants who gave the correct response afterwards rejected the scientific consensus, thereby contradicting their own initial response.

For RQ3, we got `r exp2_descriptives$justifications_n` answers from `r exp2_descriptives$justifications_n_participants` different participants to the open-ended questions on why they had rejected the scientific consensus on a particular question. Table \@ref(tab:exp2-justifications) summarizes these answers by five categories. All answers can be read in Appendix \@ref(exp2).

```{r exp2-justifications}
exp2_justifications %>%
  group_by(category)%>%
  summarize(n = n(), 
            n_subjects = n_distinct(id)) %>%
  mutate(Share = n/sum(n)) %>%
  mutate_if(is.numeric, round, digits=3) %>% 
  mutate(Share = paste0(Share*100, "%")) %>% 
  arrange(desc(n)) %>% 
  select(category, n, Share, n_subjects) %>% 
  rename(Category = category,
         `N (instances)` = n, 
         `Share (instances)` = Share, 
         `N (unique participants)` = n_subjects
         ) %>% 
  apa_table(caption = "Justifications by category")
```

## Discussion

Similar to experiment 1, most people (i) do know and agree with the scientific consensus, and (ii) tend to accept the scientific consent even if they were not previously aware of it (i.e. answered the knowledge question wrongly). The share of these latter is considerably larger in experiment 2 (`r exp2_descriptives$conditional_acceptance$false$Yes$share`) than in experiment 1 (`r exp1_descriptives$conditional_acceptance$false$Yes$share`). While this could be just sampling variation, it might be that adding explanations and sources convinced people more than merely stating the consensus. We also show, again, that people with lower trust in science and who believe more in conspiracy theories tend to both know less about science and accept the scientific consensus less.

# Experiment 3

```{r exp3}
# Analyze data of experiments and store results

# Experiment 3

# read the cleaned long version data set
exp3_long <- read_csv("exp_3/data/cleaned_long.csv")

# read wide version data set
exp3_wide <- read_csv("exp_3/data/cleaned_wide.csv") 

# read coded justifications
exp3_justifications <- read_csv("exp_3/data/justifications_clean.csv")

# Research questions

# H1
cor_trust_knowledge <- cor.test(exp3_wide$wgm_sciencegeneral, exp3_wide$avg_knowledge,
  use = "complete.obs") %>% text_ready()
cor_trust_acceptance <- cor.test(exp3_wide$wgm_sciencegeneral, exp3_wide$avg_acceptance,
  use = "complete.obs") %>% text_ready()

# H2
cor_conspiracy_knowledge <- cor.test(exp3_wide$BCTI_avg, exp3_wide$avg_knowledge,
  use = "complete.obs") %>% text_ready()
cor_conspiracy_acceptance <- cor.test(exp3_wide$BCTI_avg, exp3_wide$avg_acceptance,
  use = "complete.obs") %>% text_ready()

# Exploratory: Acceptance and trust in science/conspiracy thinking conditional on false answers only
exp3_false_knowledge <- exp3_long %>% 
  # make numeric versions
  mutate(acceptance_num = ifelse(acceptance == "Yes", 1, 0)
  ) %>% 
  group_by(id, knowledge) %>% 
  # calculate by-participant averages
  summarize(
    n = n(),
    n_accepted = sum(acceptance_num),
    avg_acceptance  = sum(acceptance_num)/n(), 
    wgm_sciencegeneral = mean(wgm_sciencegeneral), 
    BCTI_avg = mean(BCTI_avg), 
    # add additonal measures for robustness checks
    CMQ_avg = mean(CMQ_avg), 
    SICBS = mean(SICBS),
    wgm_scientists = mean(wgm_scientists), 
    pew = mean(pew)
  ) %>% 
  # filter to only false responses
  filter(knowledge == FALSE) %>% 
  ungroup()

# H1b conditional on false answers
false_answers_cor_trust_acceptance <- cor.test(exp3_false_knowledge$wgm_sciencegeneral, exp3_false_knowledge$avg_acceptance,
  use = "complete.obs") %>% 
  text_ready()

# H2b conditional on false answers
false_answers_cor_conspiracy_acceptance <- cor.test(exp3_false_knowledge$BCTI_avg, exp3_false_knowledge$avg_acceptance,
  use = "complete.obs") %>% 
  text_ready()

# extract descriptives for inline reporting
exp3_descriptives <- list(
  # Demographics
  n_subj = n_distinct(exp3_wide$id),
  n_subject_by_outcome = exp3_wide %>% 
    summarize(across(c("wgm_sciencegeneral", "reason_agreement", "BCTI_avg"), 
                     ~sum(!is.na(.x)
                     )
    )
    ),
  gender = exp3_wide %>% group_by(gender) %>% summarize(n = n_distinct(id)) %>% split(.$gender),
  age = exp3_wide %>% summarize(across(age,list(mean = mean, median = median, sd = sd), 
                                       .names = "{.fn}")) %>% rounded_numbers(),
  # RQ1 & RQ2
  means = exp3_wide %>% 
    summarize(across(c(avg_knowledge, avg_acceptance, avg_acceptance_initial), 
                     list(mean = mean, sd = sd), 
                     .names = "{.col}_{.fn}")) %>%
    rename_with(~sub("^avg_", "", .), everything()) %>% 
    mutate_if(is.numeric, round, digits = 2) %>% 
  mutate(across(ends_with("_mean"), ~paste0(.*100, " %"))) ,
  # H1
  # a
  cor_trust_knowledge = cor_trust_knowledge,
  # b (conditional on false responses)
  false_answers_cor_trust_acceptance = false_answers_cor_trust_acceptance,
  # b (all cases)
  cor_trust_acceptance = cor_trust_acceptance,
  # H2
  # a
  cor_conspiracy_knowledge = cor_conspiracy_knowledge,
  # b (conditional on false responses)
  false_answers_cor_conspiracy_acceptance = false_answers_cor_conspiracy_acceptance,
  # b(all cases)
  cor_conspiracy_acceptance = cor_conspiracy_acceptance, 
  # Exploratory
  # acceptance and knowledge
  conditional_acceptance = exp3_long %>% 
    pivot_longer(c(acceptance, acceptance_initial), 
                 names_to = "measure", 
                 values_to = "acceptance") %>% 
    group_by(knowledge, measure, acceptance) %>% 
    count() %>% 
    group_by(knowledge, measure) %>% 
    mutate (share = n/sum(n), 
            # rename knowledge values
            knowledge = ifelse(knowledge == FALSE, "false", "true")) %>% 
    mutate_if(is.numeric, round, digits = 3) %>% 
    mutate (share = paste0(share*100, " %")) %>% 
    super_split(knowledge,  measure, acceptance),
  # Justifications
  justifications_n = nrow(exp3_justifications), 
  justifications_n_participants = n_distinct(exp3_justifications$id),
  justification_by_category = exp3_justifications %>%
    group_by(category)%>%
    summarize(n = n(), 
              n_subjects = n_distinct(id)) %>%
    mutate(share = n/sum(n)) %>%
    mutate_if(is.numeric, round, digits=3) %>% 
    mutate(share = paste0(share*100, "%")) %>% 
    split(.$category),
  # Consensus acceptance
  acceptance_n = exp3_wide %>% 
    filter(!is.na(reason_agreement)) %>% 
    nrow(.),
  acceptance_by_reason = exp3_wide %>% 
    group_by(reason_agreement) %>% 
    summarize(n = n()) %>%
    drop_na(reason_agreement) %>% 
    mutate(share = n/sum(n)) %>%
    mutate_if(is.numeric, round, digits=3) %>% 
    mutate(share = paste0(share*100, "%")) %>% 
    split(.$reason_agreement),
  reason_followup_n = exp3_wide %>% 
    filter(!is.na(reason_followup)) %>% 
    nrow(.)
)

```

(ref:summary-plot) Points represent the average share of acceptance as a function of the **A** level of trust in science ("In general, would you say that you trust science a lot, some, not much, or not at all? [1 = Not at all, 2 = Not much, 3 = Some, 4 = A lot]"); **B** average conspiracy thinking (CMQ, 5 items on a scale from 0 to 100); **C** belief in a specific conspiracy theory. Participants were asked to rate their belief in the conspiracy on a scale from 1 to 9, with the labels: "1 - completly false, 5 - unsure, 9 - completely true". Averages are only based on participants who believed at least to some degree in the conspiracy theory. We consider a believer as anyone scoring higher than the scale midpoint, i.e. \>5. Number labels in plots represent the number of participants for the corresponding data point. 

```{r summary-plot, fig.cap="(ref:summary-plot)", fig.height= 10, fig.width=10}
# combine data of all three studies
combined_data <- bind_rows(exp1_long %>% 
                             mutate(study = "study 1"), 
                           exp2_long %>% 
                             mutate(study = "study 2"), 
                           exp3_long %>% 
                             mutate(study = "study 3")
                           )

# make plot data for trust
plot_trust <- combined_data %>% 
  drop_na(wgm_sciencegeneral, acceptance) %>% 
  # make a labels version of trust in science
  mutate(wgm_sciencegeneral = case_when(
    wgm_sciencegeneral == 1 ~ "1 (Not at all)",
    wgm_sciencegeneral == 2 ~ "2 (Not much)",
    wgm_sciencegeneral == 3 ~ "3 (Some)",
    wgm_sciencegeneral == 4 ~ "4 (A lot)",
    TRUE ~ as.character(wgm_sciencegeneral)  # Handle any other cases (optional)
  )) %>% 
  group_by(study, wgm_sciencegeneral, acceptance) %>% 
  summarize(n = n(), 
            n_participants = n_distinct(id)) %>% 
  group_by(study, wgm_sciencegeneral) %>% 
  mutate (share = n/sum(n), 
          n_participants = max(n_participants)) %>% 
  mutate_if(is.numeric, round, digits = 3)

# make trust plot
trust <- ggplot(plot_trust %>% 
         filter(acceptance == "Yes"), 
       aes(x = wgm_sciencegeneral, y = share, color = study, shape = study)) +
  geom_point(position = "dodge", alpha = 0.5) +
  scale_y_continuous(labels = scales::percent_format(), breaks = seq(0, 1, 0.1), limits = c(0, 1)) +
  geom_text_repel(aes(label = paste0(n_participants)),
            vjust = -0.5, size = 3,
            show.legend = FALSE) +
  scale_color_viridis_d(option = "plasma", end = 0.8) +
  labs(x = "Trust in science", y = "Acceptance rate of consensus", color = "Study", shape = "Study") +
  plot_theme + 
  theme(legend.position = "top")

# make plot data for conspiracy belief
plot_conspiracy <- combined_data %>% 
  # bring to long format
  pivot_longer(cols = BCTI_apollo:BCTI_evolution, 
               names_to = "BCTI_item", values_to = "score") %>% 
  # make a belief variable if participant indicates a numeric answer above the
  # scale midpoint
  mutate(belief = ifelse(score > 5, TRUE, 
                         ifelse(is.na(score), NA, FALSE)
                         ),
         BCTI_item = sub("^BCTI_", "", BCTI_item)
         ) %>% 
  group_by(study, BCTI_item, belief, acceptance) %>% 
  summarize(n = n(),
            n_participants = n_distinct(id)) %>% 
  group_by(study, BCTI_item, belief) %>% 
  mutate (share = n/sum(n), 
          n_participants = max(n_participants)) %>% 
  mutate_if(is.numeric, round, digits = 3) %>% 
  ungroup()

# make conspiracy belief plot
conspiracy <- ggplot(plot_conspiracy %>% 
         filter(acceptance == "Yes" & belief == TRUE), 
       aes(x = BCTI_item, y = share, color = study, shape = study)) +
  geom_point(position = "dodge", alpha = 0.5) +
  scale_y_continuous(labels = scales::percent_format(), breaks = seq(0, 1, 0.1), limits = c(0, 1)) +
  geom_text_repel(aes(label = paste0(n_participants)),
            vjust = -0.5, size = 3,
            show.legend = FALSE) +
  scale_color_viridis_d(option = "plasma", end = 0.8) +
  labs(x = "Conspiracy theory belief", y = "Acceptance rate of consensus", color = "Study", shape = "Study") +
  plot_theme +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) 

# Conspiracy Thinking

# Define breaks for each integer from 1 to 9
breaks <- seq(0, 100, by = 20)

# Define labels for each bin in the desired format
labels <- paste0(c(0, 20, 40, 60, 80), "-", c(20, 40, 60, 80, 100))

plot_data <- combined_data %>% 
  drop_na(CMQ_avg) %>% 
  # Create the binned version of CMQ_avg
  mutate(CMQ_avg_binned = cut(CMQ_avg, breaks = breaks, labels = labels, include.lowest = TRUE, right = FALSE)) %>% 
  group_by(study, CMQ_avg_binned, acceptance) %>% 
  summarize(n = n()) %>% 
  group_by(study, CMQ_avg_binned) %>% 
  mutate (share = n/sum(n)) %>% 
  mutate_if(is.numeric, round, digits = 3)

n_participants <- combined_data %>% 
  drop_na(CMQ_avg) %>% 
  # Create the binned version of CMQ_avg
  mutate(CMQ_avg_binned = cut(CMQ_avg, breaks = breaks, labels = labels, include.lowest = TRUE, right = FALSE)) %>% 
  group_by(study, CMQ_avg_binned) %>% 
  summarize(n_participants = n_distinct(id))

plot_conspiracy_thinking <- left_join(plot_data, n_participants)

# make plot
conspiracy_thinking  <- ggplot(plot_conspiracy_thinking  %>% 
         filter(acceptance == "Yes"), 
       aes(x = CMQ_avg_binned, y = share, color = study, shape = study)) +
  geom_point(position = "dodge", alpha = 0.5) +
  scale_y_continuous(labels = scales::percent_format(), breaks = seq(0, 1, 0.1), limits = c(0, 1)) +
  geom_text_repel(aes(label = paste0(n_participants)),
            vjust = -0.5, size = 3,
            show.legend = FALSE) +
  scale_color_viridis_d(option = "plasma", end = 0.8) +
  labs(x = "Conspiracy Thinking", y = "Acceptance rate of consensus", color = "Study", shape = "Study") +
  plot_theme + 
  theme(legend.position = "top")

# remove elements of plots for better overall plot
conspiracy_thinking <- conspiracy_thinking + 
  #rremove("ylab") +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) 
  #guides(color = "none", shape = "none") +

conspiracy <- conspiracy +
  rremove("ylab") +
  guides(color = "none", shape = "none")

trust <- trust +
  rremove("ylab") 

# common plot
patchwork <- (trust + conspiracy_thinking)/conspiracy + 
  plot_layout(guides = 'collect', widths = c(1, 1)) +
  plot_annotation(tag_levels = 'A')  & theme(legend.position = "top")

wrap_elements(patchwork) +
  labs(tag = "Acceptance rate of consensus") +
    theme(
    plot.tag = element_text(angle = 90, , face = "bold", size = 12),
    plot.tag.position = "left"
  )
```

Study 3 is essentially a replication-–with some minor modifications-–of study 2, but on a different type of sample. Both study 1 and 2 were run on convenience samples. For study 3, we recruited a sample of people holding anti-vaccination beliefs (see below). By contrast to study 2, after asking participants an open question about why they did not accept the consensus (in cases where they didn't), we provide them with an explicit opportunity to change their answer (Fig. \@ref(fig:exp3-explanation-example)). Based on the answers to the open-ended questions, we also pre-registered a categorization scheme of reasons why people rejected the consensus. Finally, we also ask participants about why they agree with the scientific consensus on certain questions, in case they do. We want to know if participants perceive that this is because of trust, or other factors.

As for study 2 (but without conditioning on wrong answers, as we did in study 1), we had the following hypotheses:

**H1a: Higher trust in science is associated with more science knowledge?**

**H1b: Higher trust in science is associated with more acceptance of the scientific consensus.**

**H2a: Higher conspiracy thinking is associated with less science knowledge?**

**H2b: Higher conspiracy thinking is associated with less acceptance of the scientific consensus.**

We had the following research questions:

**RQ1: What is the average science knowledge score?**

**RQ2: What is the average acceptance of the scientific consensus**

**RQ3: What reasons do participants provide to justify their rejection of the scientific consensus?**

**RQ4: In case they agree with the scientific consensus, do people feel that this is because of trust?**

## Methods

### Participants

We recruited 200 participants from the US via prolific, of which none failed our attention check, resulting in a final sample of `r exp3_descriptives$n_subj` participants (`r exp3_descriptives$gender$female$n` female, `r exp3_descriptives$gender$male$n` male; $age_\text{mean}$: `r exp3_descriptives$age$mean`, $age_\text{sd}$: `r exp3_descriptives$age$sd`, $age_\text{median}$: `r exp3_descriptives$age$median`). Since we did not have any prior assumptions on effect sizes and our analyses were descriptive, we did not do a power analysis.

However, due to a randomization mistake for our outcomes, participant answered only two of the three outcome measure blocs (trust in science, conspiracy thinking, and reason for accepting consensus). This leaves us with reduced sample sizes for all analyses concerning these outcomes (N = `r exp3_descriptives$n_subject_by_outcome$wgm_sciencegeneral` for trust in science, N = `r exp3_descriptives$n_subject_by_outcome$BCTI_avg` for conspiracy measures, N = `r exp3_descriptives$n_subject_by_outcome$reason_agreement` for reason for accepting consensus).

### Procedure

The procedure was mostly the same as in experiment 2. In addition, after each open-ended question on cases where participants rejected the scientific consensus, participants were also asked if they want to change their answer and accept the scientific consensus. Finally, at the end of the survey, we asked participants: "For the questions in which you agreed with the scientific consensus, would you say that...?" The answer options were: (i) "You mostly agree with the consensus because, on that question, you trust scientists", (ii) "You mostly agree with the consensus because you have been able to independently verify it", and (iii) "Other", with a text box for participants to explain. Participants who selected “You mostly agree with the consensus because you have been able to independently verify it”, were asked the open-ended follow-up question: "Could you please tell us how you independently verified the information?".

### Materials

We relied on the same items as in experiment 2. We used a pre-registered categorization scheme for the open ended answers for why people reject the consensus. The categories were XX and XX. [Describe coding process, with independent reviewer etc.]

## Results

As in study 1 and 2, we find that participants answered on average `r exp3_descriptives$means$knowledge_mean` (sd = `r exp3_descriptives$means$knowledge_sd`) of the questions correctly, and initially accepted the scientific consensus on average for `r exp3_descriptives$means$acceptance_initial_mean` (sd = `r exp3_descriptives$means$acceptance_initial_sd`) of the questions (RQ2). The acceptance rate is even higher when accounting for opinion revisions towards acceptance of the consensus, after initial rejection (`r exp3_descriptives$means$acceptance_mean`, sd = `r exp3_descriptives$means$acceptance_sd`).

Fig. \@ref(fig:exp3-conditional-acceptance) illustrates the relationship between knowledge and acceptance. In most cases (`r exp3_descriptives$conditional_acceptance$false$acceptance_initial$Yes$share`), participants readily accepted the scientific consensus right after having given the wrong answer to a question. After providing the chance to revise the initial consensus rejection, this share is even larger (`r exp3_descriptives$conditional_acceptance$false$acceptance$Yes$share`). In very few cases (`r exp3_descriptives$conditional_acceptance$true$acceptance_initial$No$share`), participants who initially gave the correct response afterwards rejected the scientific consensus right after, thereby contradicting their own initial response. This share drops slightly after providing the chance to revise the initial consensus rejection (`r exp3_descriptives$conditional_acceptance$true$acceptance$No$share`).

In the appendix XX we provide an analysis that this drop is statistically significant [TO DO].

For all correlations, we include opinion revisions for measuring consensus acceptance. We find no statistically significant correlation between science knowledge and trust in science (r = `r exp3_descriptives$cor_trust_knowledge$estimate`, p `r exp3_descriptives$cor_trust_knowledge$p.value`), but a samll positive correlation between acceptance of scientific consensus and trust in science (r = `r exp3_descriptives$cor_trust_acceptance$estimate`, p `r exp3_descriptives$cor_trust_acceptance$p.value`). Again, these findings might be partly due to ceiling effects: As illustrated in Fig. \@ref(fig:exp3-plot), (i) most people do trust science, and (ii) that is true even among people with low knowledge or acceptance rates. We find no statistically significant correlation between conspiracy thinking and science knowledge (r = `r exp3_descriptives$cor_conspiracy_knowledge$estimate`, p `r exp3_descriptives$cor_conspiracy_knowledge$p.value`), and between conspiracy thinking and acceptance of scientific consensus (r = `r exp3_descriptives$cor_conspiracy_acceptance`, p `r exp3_descriptives$cor_conspiracy_acceptance$p.value`).

In Appendix \@ref(exp3), we show that these associations are statistically significant (except the last one?). We show that they are robust when using alternative measures of trust and conspiracy thinking. Appendix \@ref(exp3) also includes more descriptive statistics, such as knowledge and acceptance by science questions.

Regarding RQ1, participants answered on average `r exp3_descriptives$means$knowledge_mean` (sd = `r exp3_descriptives$means$knowledge_sd`) of the questions correctly. For RQ3, we got `r exp3_descriptives$justifications_n` answers from `r exp3_descriptives$justifications_n_participants` different participants to the open-ended questions on why they had rejected the scientific consensus on a particular question. Table \@ref(tab:exp3-justifications) summarizes these answers by five categories.

All answers can be read in the appendix.

```{r exp3-justifications}
exp3_justifications %>%
  group_by(category)%>%
  summarize(n = n(), 
            n_subjects = n_distinct(id)) %>%
  mutate(Share = n/sum(n)) %>%
  mutate_if(is.numeric, round, digits=3) %>% 
  mutate(Share = paste0(Share*100, "%")) %>% 
  arrange(desc(n)) %>% 
  select(category, n, Share, n_subjects) %>% 
  rename(Category = category,
         `N (instances)` = n, 
         `Share (instances)` = Share, 
         `N (unique participants)` = n_subjects
         ) %>% 
  apa_table(caption = "Justifications by category")
```

For RQ4, we had `r exp3_descriptives$acceptance_n` participants answering the question. Of these `r exp3_descriptives$acceptance_by_reason[["trust in scientists"]]$share` said they accepted the scientific consensus because they trust scientists on this question, while `r exp3_descriptives$acceptance_by_reason[["independent verification"]]$share` said they independently verified the fact. `r exp3_descriptives$acceptance_by_reason[["other"]]$share` answered with other "other" and gave an open-ended explanation (see Appendix).

We also asked all `r exp3_descriptives$reason_followup_n` participants who answered that they had independently verified the answer to explain how they did so. The open-ended answers are listed in Appendix XX.

# Discussion

[We can draw some inspiration from the “Westwood et al 2021 Current research overstates American support for political violence” paper: we want to put in perspective the current rise (? discourse around?) in mistrust towards science]

There is increasingly more talk of defiance towards science and towards experts more generally. In every population, many people only trust science ‘some of the time’, and a sizable minority doesn’t trust it. This appears difficult to reconcile with the fact that, in many countries, most people receive at least a basic science education, that science education (along with education more generally) is increasing, and that science education seems to be the main driver of trust in science.

But how deep is distrust of science? In four studies, we have shown that almost every participant accepts almost all of basic science questions we included.

Traditionally, people have interpreted people's lack of basic knowledge in light of the deficit-model. Here, we show that people readily accept the scientifically consensual answer to these questions.

“The dominant approach to conceptualizing and measuring science literacy in population surveys has arisen out of work by Jon D. Miller and Kenneth Prewitt in the United States (see Miller, 1983, 1998, 2004) alongside collabora- tors in Great Britain (see Durant et al., 1989). Underlying these efforts appears to have been widespread concern among policy makers and the scientific com- munity that nonscientists were becoming skeptical about the benefits of sci- ence and that such skepticism might result in cuts to science funding that would harm the scientific progress that many argue underpins both American and European economic development (Bauer et al., 2007). The results of the U.S. portion of this work have formed the core of a chapter of a biennial report called Science and Engineering Indicators (hereafter, Indicators) that the National Science Board provides to Congress and the Executive Branch. Scholars have also used the raw data collected for Indicators (which is made publicly available) for peer-reviewed research (e.g., Gauchat, 2012; Losh, 2010), and other countries have used many of the Indicators’ questions for their own national surveys (e.g., Bauer et al., 2012a; National Science Board, 2016).”

\FloatBarrier

# References

::: {#refs}
:::

\newpage

# (APPENDIX) Appendix {.unnumbered}

```{r child = "appendix_exp1.Rmd"}
```

\clearpage

```{r child = "appendix_exp2.Rmd"}
```

\clearpage

```{r child = "appendix_exp3.Rmd"}
```
